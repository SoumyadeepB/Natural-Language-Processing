{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Speaker Encoder</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-08 20:33:15,898 [nnabla][INFO]: Initializing CPU extension...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import pandas as pd\n",
    "import librosa as lr\n",
    "import librosa.display as ld\n",
    "import glob\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import nnabla as nn\n",
    "import nnabla.parametric_functions as PF\n",
    "import nnabla.functions as F\n",
    "import nnabla.solvers as S\n",
    "import tensorflow as tf\n",
    "from nnabla.utils.data_iterator import data_iterator_simple\n",
    "import nnabla.monitor as M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Hyperparameters (to be moved to hparams.py)  #######################\n",
    "\n",
    "### Directory Locations ###\n",
    "#basedir = 'drive/My Drive/Colab Notebooks/SV2TTS'\n",
    "data_dir = \"./data/LJSpeech/\"\n",
    "label_dir = \"./data/LJSpeech/labels/\"\n",
    "save_dir_mfcc = \"./data/LJSpeech/mfcc/\"\n",
    "save_dir_transcripts = \"./data/LJSpeech/transcripts/\"\n",
    "\n",
    "### FFT Parameters ###\n",
    "sr = 22500\n",
    "n_fft = 2048\n",
    "hop_length = 512\n",
    "n_mfcc = 13\n",
    "mel_len = 290                      # frame length of mel spectrogram > Spectrogram is split into short-time frames\n",
    "n_fft = 1024 \n",
    "n_mels = 80                        # number of mel filters (number of Mel bands to generate)\n",
    "hop_length = 256                   # audio samples between adjacent STFT columns\n",
    "win_length = 1024                  # window length\n",
    "mel_fmin = 0.0                     # minimum mel bank\n",
    "mel_fmax = 8000                    # maximum mel bank\n",
    "r = 3                              # number of frames generated on each timestep\n",
    "\n",
    "### Model Parameters ###\n",
    "batch_size = 20\n",
    "lstm_layers = 3\n",
    "lstm_hidden = 256\n",
    "lstm_directions = 1\n",
    "affine_hidden = 256\n",
    "embed_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = np.load(label_dir + 's_id.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9725"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 9725/9725 [02:24<00:00, 67.23it/s]\n"
     ]
    }
   ],
   "source": [
    "xs = []\n",
    "mfccs = sorted(glob.glob(save_dir_mfcc + '*.npy'))\n",
    "for i in tqdm(range(len(mfccs))):\n",
    "    xs.append((np.load(mfccs[i])).T)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9725, 141, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(xs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LJ001': 0, 'LJ002': 1, 'LJ003': 2, 'LJ004': 3, 'LJ005': 4, 'LJ006': 5, 'LJ007': 6, 'LJ008': 7, 'LJ009': 8, 'LJ010': 9}\n"
     ]
    }
   ],
   "source": [
    "idx = {k: i for i, k in enumerate(sorted(set(ys)))}\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9725, 141, 13)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(xs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Get total utterances for each speaker\n",
    "'''\n",
    "\n",
    "utter_count = {k: 0 for k in sorted(set(ys))}\n",
    "mfccs = {k:[] for k in sorted(set(ys))}\n",
    "\n",
    "for i, s_id in enumerate(ys):\n",
    "    utter_count[s_id]+=1\n",
    "    mfccs[s_id].append(xs[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LJ001': 598, 'LJ002': 1130, 'LJ003': 1201, 'LJ004': 816, 'LJ005': 989, 'LJ006': 1049, 'LJ007': 850, 'LJ008': 1031, 'LJ009': 973, 'LJ010': 1088}\n",
      "(598, 141, 13)\n",
      "(1130, 141, 13)\n",
      "(1201, 141, 13)\n",
      "(816, 141, 13)\n",
      "(989, 141, 13)\n",
      "(1049, 141, 13)\n",
      "(850, 141, 13)\n",
      "(1031, 141, 13)\n",
      "(973, 141, 13)\n",
      "(1088, 141, 13)\n"
     ]
    }
   ],
   "source": [
    "print(utter_count)\n",
    "for key in mfccs:\n",
    "    print(np.array(mfccs[key]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 1201/1201 [00:00<00:00, 109484.01it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Create customized dataset\n",
    "'''\n",
    "n_speakers = len(set(utter_count)) #number of speakers\n",
    "n_utterances = 2 #utterances per batch\n",
    "max_utter = utter_count[max(utter_count, key=utter_count.get)] \n",
    "\n",
    "dataset = []\n",
    "labels = []\n",
    "\n",
    "## Batch_size =20 , each batch will contain 2 utterances from each of the 10 speakers\n",
    "def split_dataset(xs, ys):\n",
    "    \n",
    "    for i in tqdm(range(max_utter)):\n",
    "        \n",
    "        for s_id in utter_count:\n",
    "            max_idx = utter_count[s_id] #Allow repeating data\n",
    "            \n",
    "            ## Appending n_utterances for each speaker (in one batch)\n",
    "            for j in range(n_utterances):\n",
    "                data = mfccs[s_id][(i+j)%max_idx]\n",
    "                dataset.append(data)\n",
    "                labels.append(s_id)\n",
    "                \n",
    "                \n",
    "    return dataset,labels\n",
    "\n",
    "dataset, labels = split_dataset(xs,ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_network(inputs, training = False):\n",
    "    with nn.parameter_scope('encoder_network/lstm'):\n",
    "        h = nn.Variable((lstm_layers, lstm_directions, batch_size, lstm_hidden))\n",
    "        c = nn.Variable((lstm_layers, lstm_directions, batch_size, lstm_hidden))\n",
    "        \n",
    "        y, hn, cn = PF.lstm(inputs, h, c, training = training)\n",
    "    with nn.parameter_scope('encoder_network/dense'):\n",
    "        #out = PF.affine(hn[-1], affine_hidden)\n",
    "        #\n",
    "        out = F.relu(hn[-1]) \n",
    "        #out.d = out.d/(np.linalg.norm(out.d))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(dataset,labels, batch_size):\n",
    "    batch_data = dataset[:batch_size]\n",
    "    batch_labels = labels[:batch_size]\n",
    "    del dataset[:batch_size]\n",
    "    del labels[:batch_size]\n",
    "    dataset += batch_data\n",
    "    labels += batch_labels\n",
    "    batch_data = np.transpose(batch_data, (2,0,1) ) #convert shape from (B,I,T)  -> (T,B,I)\n",
    "    return batch_data, np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Trainable parameteres for ths similarity matrix\n",
    "'''\n",
    "\n",
    "sim_weight = nn.Variable([1], need_grad = True)\n",
    "sim_weight.d = 10.0\n",
    "sim_bias = nn.Variable([1], need_grad = True)\n",
    "sim_bias.d = -5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_matrix(embeddings):\n",
    "    \"\"\"\n",
    "        Computes the similarity matrix according the section 2.1 of GE2E.\n",
    "        :param embeds: the embeddings as a tensor of shape (speakers_per_batch, \n",
    "        utterances_per_speaker, embedding_size)\n",
    "        :return: the similarity matrix as a tensor of shape (speakers_per_batch,\n",
    "        utterances_per_speaker, speakers_per_batch)\n",
    "        \"\"\"\n",
    "    embeddings = np.reshape(embeddings, [n_speakers, n_utterances, embed_size])\n",
    "    centroids_incl = embeddings.mean(axis = 1, keepdims = True) # one centroid per speaker\n",
    "    centroids_incl = centroids_incl/np.linalg.norm(centroids_incl, axis = 2, keepdims = True)\n",
    "    centroids_excl = np.sum(embeddings, axis = 1, keepdims = True) - embeddings\n",
    "    centroids_excl /= (n_utterances - 1)\n",
    "    centroids_excl = centroids_excl/np.linalg.norm(centroids_excl, axis = 2, keepdims = True)\n",
    "    sim_matrix = np.zeros((n_speakers, n_utterances, n_speakers), dtype = float)\n",
    "    mask_matrix = 1 - np.eye(n_speakers, dtype = np.int)\n",
    "    \n",
    "    for j in range(n_speakers):\n",
    "        mask = np.where(mask_matrix[j])[0]\n",
    "        sim_matrix[mask, :, j] = (embeddings[mask] * centroids_incl[j]).sum(axis=2)\n",
    "        sim_matrix[j, :, j] = (embeddings[j] * centroids_excl[j]).sum(axis=1)\n",
    "    sim_matrix = sim_matrix * sim_weight.d + sim_bias.d\n",
    "    return sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(targets, nb_classes):\n",
    "    res = np.eye(nb_classes)[np.array(targets).reshape(-1)]\n",
    "    return res.reshape(list(targets.shape)+[nb_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(embeddings):\n",
    "    sim_matrix = similarity_matrix(embeddings.d)\n",
    "    sim_matrix = sim_matrix.reshape((n_speakers * n_utterances, n_speakers))\n",
    "    ground_truth = np.repeat(np.arange(n_speakers), n_utterances)\n",
    "    ground_truth = ground_truth.reshape((n_speakers*n_utterances,1))\n",
    "    #ground_truth = get_one_hot(ground_truth, n_speakers)\n",
    "    shape1 = sim_matrix.shape\n",
    "    shape2 = ground_truth.shape\n",
    "    sm = nn.Variable(shape1)\n",
    "    gt = nn.Variable(shape2)\n",
    "    sm.d = sim_matrix\n",
    "    gt.d = ground_truth\n",
    "    loss = F.softmax_cross_entropy(sm,gt)    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(xs):\n",
    "    embeds = encoder_network(xs, training = True)\n",
    "    return embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch = len(dataset)//batch_size\n",
    "max_epochs = 600\n",
    "\n",
    "def train():\n",
    "    monitor = M.Monitor('.')\n",
    "    monitor_loss = M.MonitorSeries(\n",
    "        \"Training loss\", monitor, interval=5000, verbose = True)\n",
    "    monitor_time = M.MonitorTimeElapsed(\n",
    "        \"Training time\", monitor, interval=10000, verbose = True)\n",
    "    optimizer = S.RMSprop()\n",
    "        \n",
    "    for epoch in tqdm(range(max_epochs)):\n",
    "        \n",
    "        #Iterations per epoch\n",
    "        \n",
    "        for i in range(n_batch):\n",
    "            xi = nn.Variable((13, batch_size, 141))            \n",
    "            xi.d, yi = generate_batch(dataset,labels, batch_size)\n",
    "            optimizer.zero_grad()\n",
    "            embeddings = encoder_network(xi, True)\n",
    "            loss = loss_fn(embeddings)\n",
    "            loss.backward()\n",
    "            optimizer.update()\n",
    "        \n",
    "            # monitor\n",
    "            itr = epoch * n_batch + i\n",
    "            monitor_loss.add(itr, loss.d)\n",
    "            #monitor_time.add(itr)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                                 | 4/600 [00:14<35:12,  3.55s/it]2020-12-08 20:37:33,724 [nnabla][INFO]: iter=4999 {Training loss}=8.411543252355852e+30\n",
      "  1%|█                                                                                 | 8/600 [00:28<35:41,  3.62s/it]2020-12-08 20:37:48,827 [nnabla][INFO]: iter=9999 {Training loss}=1.8692375964032627e+31\n",
      "  2%|█▌                                                                               | 12/600 [00:43<35:11,  3.59s/it]2020-12-08 20:38:03,758 [nnabla][INFO]: iter=14999 {Training loss}=3.063472912135681e+31\n",
      "  3%|██▏                                                                              | 16/600 [00:57<35:12,  3.62s/it]2020-12-08 20:38:18,821 [nnabla][INFO]: iter=19999 {Training loss}=2.4819372804973577e+31\n",
      "  3%|██▋                                                                              | 20/600 [01:12<35:04,  3.63s/it]2020-12-08 20:38:33,866 [nnabla][INFO]: iter=24999 {Training loss}=2.5130915406539906e+31\n",
      "  4%|███▏                                                                             | 24/600 [01:26<34:24,  3.58s/it]2020-12-08 20:38:48,814 [nnabla][INFO]: iter=29999 {Training loss}=2.0042377382167186e+31\n",
      "  5%|███▉                                                                             | 29/600 [01:44<33:59,  3.57s/it]2020-12-08 20:39:03,675 [nnabla][INFO]: iter=34999 {Training loss}=2.554624670757179e+31\n",
      "  6%|████▍                                                                            | 33/600 [01:58<33:51,  3.58s/it]2020-12-08 20:39:18,603 [nnabla][INFO]: iter=39999 {Training loss}=6.749998628192054e+30\n",
      "  6%|████▉                                                                            | 37/600 [02:12<33:28,  3.57s/it]2020-12-08 20:39:33,428 [nnabla][INFO]: iter=44999 {Training loss}=1.3396172289143965e+31\n",
      "  7%|█████▌                                                                           | 41/600 [02:27<33:32,  3.60s/it]2020-12-08 20:39:48,464 [nnabla][INFO]: iter=49999 {Training loss}=2.7000103316091983e+31\n",
      "  8%|██████                                                                           | 45/600 [02:41<33:06,  3.58s/it]2020-12-08 20:40:03,312 [nnabla][INFO]: iter=54999 {Training loss}=1.0384615970976143e+31\n",
      "  8%|██████▌                                                                          | 49/600 [02:55<32:44,  3.56s/it]2020-12-08 20:40:18,113 [nnabla][INFO]: iter=59999 {Training loss}=2.3573156459527115e+31\n",
      "  9%|███████▎                                                                         | 54/600 [03:13<32:22,  3.56s/it]2020-12-08 20:40:32,887 [nnabla][INFO]: iter=64999 {Training loss}=5.088460653120265e+30\n",
      " 10%|███████▊                                                                         | 58/600 [03:27<32:18,  3.58s/it]2020-12-08 20:40:47,827 [nnabla][INFO]: iter=69999 {Training loss}=2.0353924819436793e+31\n",
      " 10%|████████▎                                                                        | 62/600 [03:42<32:19,  3.61s/it]2020-12-08 20:41:02,898 [nnabla][INFO]: iter=74999 {Training loss}=1.9627002559805795e+31\n",
      " 11%|████████▉                                                                        | 66/600 [03:57<33:03,  3.71s/it]2020-12-08 20:41:18,493 [nnabla][INFO]: iter=79999 {Training loss}=3.6450155555437577e+31\n",
      " 12%|█████████▍                                                                       | 70/600 [04:11<31:39,  3.58s/it]2020-12-08 20:41:33,126 [nnabla][INFO]: iter=84999 {Training loss}=1.0592320307118313e+31\n",
      " 12%|█████████▉                                                                       | 74/600 [04:25<30:58,  3.53s/it]2020-12-08 20:41:47,817 [nnabla][INFO]: iter=89999 {Training loss}=6.126920074151404e+30\n",
      " 13%|██████████▋                                                                      | 79/600 [04:43<30:57,  3.56s/it]2020-12-08 20:42:02,668 [nnabla][INFO]: iter=94999 {Training loss}=8.515377891002552e+30\n",
      " 14%|███████████▏                                                                     | 83/600 [04:57<30:40,  3.56s/it]2020-12-08 20:42:17,491 [nnabla][INFO]: iter=99999 {Training loss}=1.8900076673397338e+31\n",
      " 14%|███████████▋                                                                     | 87/600 [05:12<30:36,  3.58s/it]2020-12-08 20:42:32,442 [nnabla][INFO]: iter=104999 {Training loss}=1.8484686135000293e+31\n",
      " 15%|████████████▎                                                                    | 91/600 [05:26<30:28,  3.59s/it]2020-12-08 20:42:47,360 [nnabla][INFO]: iter=109999 {Training loss}=5.815380494899624e+30\n",
      " 16%|████████████▊                                                                    | 95/600 [05:40<30:04,  3.57s/it]2020-12-08 20:43:02,172 [nnabla][INFO]: iter=114999 {Training loss}=5.503839706722024e+30\n",
      " 16%|█████████████▎                                                                   | 99/600 [05:55<29:53,  3.58s/it]2020-12-08 20:43:17,147 [nnabla][INFO]: iter=119999 {Training loss}=5.711536184846366e+30\n",
      " 17%|█████████████▊                                                                  | 104/600 [06:12<29:33,  3.57s/it]2020-12-08 20:43:32,039 [nnabla][INFO]: iter=124999 {Training loss}=8.203852818860607e+30\n",
      " 18%|██████████████▍                                                                 | 108/600 [06:27<29:36,  3.61s/it]2020-12-08 20:43:47,139 [nnabla][INFO]: iter=129999 {Training loss}=6.749993792488776e+30\n",
      " 19%|██████████████▉                                                                 | 112/600 [06:41<29:07,  3.58s/it]2020-12-08 20:44:02,025 [nnabla][INFO]: iter=134999 {Training loss}=1.9419300641515265e+31\n",
      " 19%|███████████████▍                                                                | 116/600 [06:56<28:50,  3.57s/it]2020-12-08 20:44:16,857 [nnabla][INFO]: iter=139999 {Training loss}=2.0042383426796284e+31\n",
      " 20%|████████████████                                                                | 120/600 [07:10<28:27,  3.56s/it]2020-12-08 20:44:31,635 [nnabla][INFO]: iter=144999 {Training loss}=2.388471356820328e+31\n",
      " 21%|████████████████▌                                                               | 124/600 [07:24<28:33,  3.60s/it]2020-12-08 20:44:46,668 [nnabla][INFO]: iter=149999 {Training loss}=2.658473091158223e+31\n",
      " 22%|█████████████████▏                                                              | 129/600 [07:42<28:02,  3.57s/it]2020-12-08 20:45:01,517 [nnabla][INFO]: iter=154999 {Training loss}=1.0903845983723167e+31\n",
      " 22%|█████████████████▋                                                              | 133/600 [07:56<27:18,  3.51s/it]2020-12-08 20:45:16,063 [nnabla][INFO]: iter=159999 {Training loss}=7.373086853639262e+30\n",
      " 23%|██████████████████▎                                                             | 137/600 [08:10<27:20,  3.54s/it]2020-12-08 20:45:30,764 [nnabla][INFO]: iter=164999 {Training loss}=6.646151295824248e+30\n",
      " 24%|██████████████████▊                                                             | 141/600 [08:24<27:07,  3.55s/it]2020-12-08 20:45:45,516 [nnabla][INFO]: iter=169999 {Training loss}=2.2015453123102026e+31\n",
      " 24%|███████████████████▎                                                            | 145/600 [08:39<27:10,  3.58s/it]2020-12-08 20:46:00,643 [nnabla][INFO]: iter=174999 {Training loss}=3.2711675768712937e+31\n",
      " 25%|███████████████████▊                                                            | 149/600 [08:53<27:08,  3.61s/it]2020-12-08 20:46:15,732 [nnabla][INFO]: iter=179999 {Training loss}=1.1423094130357486e+31\n",
      " 26%|████████████████████▌                                                           | 154/600 [09:11<26:36,  3.58s/it]2020-12-08 20:46:30,677 [nnabla][INFO]: iter=184999 {Training loss}=1.2150001882878648e+31\n",
      " 26%|█████████████████████                                                           | 158/600 [09:25<26:04,  3.54s/it]2020-12-08 20:46:45,363 [nnabla][INFO]: iter=189999 {Training loss}=3.7073209326498926e+31\n",
      " 27%|█████████████████████▌                                                          | 162/600 [09:39<25:39,  3.52s/it]2020-12-08 20:46:59,998 [nnabla][INFO]: iter=194999 {Training loss}=1.8276987843487221e+31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██████████████████████▏                                                         | 166/600 [09:53<25:23,  3.51s/it]2020-12-08 20:47:14,633 [nnabla][INFO]: iter=199999 {Training loss}=1.9315456331462007e+31\n",
      " 28%|██████████████████████▋                                                         | 170/600 [10:08<25:19,  3.53s/it]2020-12-08 20:47:29,390 [nnabla][INFO]: iter=204999 {Training loss}=6.230772242222489e+30\n",
      " 29%|███████████████████████▏                                                        | 174/600 [10:22<24:53,  3.51s/it]2020-12-08 20:47:43,996 [nnabla][INFO]: iter=209999 {Training loss}=2.51308718852104e+31\n",
      " 30%|███████████████████████▊                                                        | 179/600 [10:39<24:43,  3.52s/it]2020-12-08 20:47:58,722 [nnabla][INFO]: iter=214999 {Training loss}=5.192303149784793e+30\n",
      " 30%|████████████████████████▍                                                       | 183/600 [10:54<24:44,  3.56s/it]2020-12-08 20:48:13,588 [nnabla][INFO]: iter=219999 {Training loss}=2.7934716613681135e+31\n",
      " 31%|████████████████████████▉                                                       | 187/600 [11:08<24:23,  3.54s/it]2020-12-08 20:48:28,319 [nnabla][INFO]: iter=224999 {Training loss}=1.952315220512344e+31\n",
      " 32%|█████████████████████████▍                                                      | 191/600 [11:22<24:05,  3.53s/it]2020-12-08 20:48:43,059 [nnabla][INFO]: iter=229999 {Training loss}=3.3023230459537463e+31\n",
      " 32%|██████████████████████████                                                      | 195/600 [11:36<23:46,  3.52s/it]2020-12-08 20:48:57,731 [nnabla][INFO]: iter=234999 {Training loss}=1.3500009345642305e+31\n",
      " 33%|██████████████████████████▌                                                     | 199/600 [11:50<23:31,  3.52s/it]2020-12-08 20:49:12,351 [nnabla][INFO]: iter=239999 {Training loss}=2.232698363541016e+31\n",
      " 34%|███████████████████████████                                                     | 203/600 [12:04<23:13,  3.51s/it]2020-12-08 20:49:26,961 [nnabla][INFO]: iter=244999 {Training loss}=1.42269436945315e+31\n",
      " 35%|███████████████████████████▋                                                    | 208/600 [12:22<23:27,  3.59s/it]2020-12-08 20:49:41,973 [nnabla][INFO]: iter=249999 {Training loss}=3.1257823995896024e+31\n",
      " 35%|████████████████████████████▎                                                   | 212/600 [12:37<23:25,  3.62s/it]2020-12-08 20:49:57,118 [nnabla][INFO]: iter=254999 {Training loss}=5.399996605594586e+30\n",
      " 36%|████████████████████████████▊                                                   | 216/600 [12:51<23:08,  3.62s/it]2020-12-08 20:50:12,118 [nnabla][INFO]: iter=259999 {Training loss}=2.1911611230900408e+31\n",
      " 37%|█████████████████████████████▎                                                  | 220/600 [13:06<23:30,  3.71s/it]2020-12-08 20:50:27,732 [nnabla][INFO]: iter=264999 {Training loss}=1.8796229945492442e+31\n",
      " 37%|█████████████████████████████▊                                                  | 224/600 [13:21<23:02,  3.68s/it]2020-12-08 20:50:43,021 [nnabla][INFO]: iter=269999 {Training loss}=2.191160155949385e+31\n",
      " 38%|██████████████████████████████▍                                                 | 228/600 [13:35<22:23,  3.61s/it]2020-12-08 20:50:57,886 [nnabla][INFO]: iter=274999 {Training loss}=6.438462071254823e+30\n",
      " 39%|███████████████████████████████                                                 | 233/600 [13:53<21:50,  3.57s/it]2020-12-08 20:51:12,678 [nnabla][INFO]: iter=279999 {Training loss}=3.3334768225400513e+31\n",
      " 40%|███████████████████████████████▌                                                | 237/600 [14:07<21:29,  3.55s/it]2020-12-08 20:51:27,538 [nnabla][INFO]: iter=284999 {Training loss}=1.6200041196131092e+31\n",
      " 40%|████████████████████████████████▏                                               | 241/600 [14:22<21:52,  3.65s/it]2020-12-08 20:51:42,808 [nnabla][INFO]: iter=289999 {Training loss}=5.296119050081289e+30\n",
      " 41%|████████████████████████████████▋                                               | 245/600 [14:36<21:23,  3.62s/it]2020-12-08 20:51:57,805 [nnabla][INFO]: iter=294999 {Training loss}=2.6145293488036237e+26\n",
      " 42%|█████████████████████████████████▏                                              | 249/600 [14:51<21:02,  3.60s/it]2020-12-08 20:52:12,685 [nnabla][INFO]: iter=299999 {Training loss}=2.693757561197884e+26\n",
      " 42%|█████████████████████████████████▋                                              | 253/600 [15:05<20:45,  3.59s/it]2020-12-08 20:52:27,685 [nnabla][INFO]: iter=304999 {Training loss}=7.60590340922809e+25\n",
      " 43%|██████████████████████████████████▍                                             | 258/600 [15:24<21:57,  3.85s/it]2020-12-08 20:52:43,858 [nnabla][INFO]: iter=309999 {Training loss}=1.0141204545637454e+26\n",
      " 44%|██████████████████████████████████▉                                             | 262/600 [15:39<20:47,  3.69s/it]2020-12-08 20:52:59,051 [nnabla][INFO]: iter=314999 {Training loss}=1.6954826753260144e+26\n",
      " 44%|███████████████████████████████████▍                                            | 266/600 [15:53<20:16,  3.64s/it]2020-12-08 20:53:14,368 [nnabla][INFO]: iter=319999 {Training loss}=1.996549771743739e+26\n",
      " 45%|████████████████████████████████████                                            | 270/600 [16:08<20:10,  3.67s/it]2020-12-08 20:53:29,573 [nnabla][INFO]: iter=324999 {Training loss}=2.7571400204328278e+26\n",
      " 46%|████████████████████████████████████▌                                           | 274/600 [16:23<19:50,  3.65s/it]2020-12-08 20:53:44,728 [nnabla][INFO]: iter=329999 {Training loss}=9.348923344032054e+25\n",
      " 46%|█████████████████████████████████████                                           | 278/600 [16:37<19:31,  3.64s/it]2020-12-08 20:54:00,112 [nnabla][INFO]: iter=334999 {Training loss}=2.852213801518964e+26\n",
      " 47%|█████████████████████████████████████▋                                          | 283/600 [16:56<19:29,  3.69s/it]2020-12-08 20:54:15,484 [nnabla][INFO]: iter=339999 {Training loss}=1.616254462931754e+26\n",
      " 48%|██████████████████████████████████████▎                                         | 287/600 [17:10<18:46,  3.60s/it]2020-12-08 20:54:30,440 [nnabla][INFO]: iter=344999 {Training loss}=1.7430195658690825e+26\n",
      " 48%|██████████████████████████████████████▊                                         | 291/600 [17:24<18:18,  3.55s/it]2020-12-08 20:54:45,210 [nnabla][INFO]: iter=349999 {Training loss}=2.155006012064819e+26\n",
      " 49%|███████████████████████████████████████▎                                        | 295/600 [17:39<18:03,  3.55s/it]2020-12-08 20:54:59,974 [nnabla][INFO]: iter=354999 {Training loss}=2.7729857735921443e+26\n",
      " 50%|███████████████████████████████████████▊                                        | 299/600 [17:53<17:46,  3.54s/it]2020-12-08 20:55:14,666 [nnabla][INFO]: iter=359999 {Training loss}=1.441952561685078e+26\n",
      " 50%|████████████████████████████████████████▍                                       | 303/600 [18:07<17:25,  3.52s/it]2020-12-08 20:55:29,414 [nnabla][INFO]: iter=364999 {Training loss}=1.1091942356498814e+26\n",
      " 51%|█████████████████████████████████████████                                       | 308/600 [18:24<17:09,  3.53s/it]2020-12-08 20:55:44,112 [nnabla][INFO]: iter=369999 {Training loss}=1.0299661154893415e+26\n",
      " 52%|█████████████████████████████████████████▌                                      | 312/600 [18:39<17:03,  3.55s/it]2020-12-08 20:55:58,949 [nnabla][INFO]: iter=374999 {Training loss}=9.348923344032054e+25\n",
      " 53%|██████████████████████████████████████████▏                                     | 316/600 [18:53<16:46,  3.55s/it]2020-12-08 20:56:13,712 [nnabla][INFO]: iter=379999 {Training loss}=1.2042681089697379e+26\n",
      " 53%|██████████████████████████████████████████▋                                     | 320/600 [19:07<16:43,  3.59s/it]2020-12-08 20:56:28,700 [nnabla][INFO]: iter=384999 {Training loss}=-6.957501678185258e+30\n",
      " 54%|███████████████████████████████████████████▏                                    | 324/600 [19:22<16:49,  3.66s/it]2020-12-08 20:56:44,038 [nnabla][INFO]: iter=389999 {Training loss}=1.9731647480983093e+30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|███████████████████████████████████████████▋                                    | 328/600 [19:36<16:22,  3.61s/it]2020-12-08 20:56:59,062 [nnabla][INFO]: iter=394999 {Training loss}=9.761717546882615e+30\n",
      " 56%|████████████████████████████████████████████▍                                   | 333/600 [19:55<16:15,  3.65s/it]2020-12-08 20:57:14,448 [nnabla][INFO]: iter=399999 {Training loss}=9.034808585435633e+30\n",
      " 56%|████████████████████████████████████████████▉                                   | 337/600 [20:09<15:47,  3.60s/it]2020-12-08 20:57:29,381 [nnabla][INFO]: iter=404999 {Training loss}=1.557787356769552e+30\n",
      " 57%|█████████████████████████████████████████████▍                                  | 341/600 [20:24<15:33,  3.61s/it]2020-12-08 20:57:44,467 [nnabla][INFO]: iter=409999 {Training loss}=4.6731985630915535e+30\n",
      " 57%|██████████████████████████████████████████████                                  | 345/600 [20:38<15:13,  3.58s/it]2020-12-08 20:57:59,344 [nnabla][INFO]: iter=414999 {Training loss}=1.4642579644869926e+31\n",
      " 58%|██████████████████████████████████████████████▌                                 | 349/600 [20:52<14:58,  3.58s/it]2020-12-08 20:58:14,235 [nnabla][INFO]: iter=419999 {Training loss}=1.869301397463393e+30\n",
      " 59%|███████████████████████████████████████████████                                 | 353/600 [21:07<14:55,  3.63s/it]2020-12-08 20:58:29,412 [nnabla][INFO]: iter=424999 {Training loss}=1.869280694608732e+30\n",
      " 60%|███████████████████████████████████████████████▋                                | 358/600 [21:25<14:42,  3.65s/it]2020-12-08 20:58:44,579 [nnabla][INFO]: iter=429999 {Training loss}=2.907839852019989e+30\n",
      " 60%|████████████████████████████████████████████████▎                               | 362/600 [21:39<14:13,  3.59s/it]2020-12-08 20:58:59,505 [nnabla][INFO]: iter=434999 {Training loss}=7.996305038612078e+30\n",
      " 61%|████████████████████████████████████████████████▊                               | 366/600 [21:54<14:09,  3.63s/it]2020-12-08 20:59:14,641 [nnabla][INFO]: iter=439999 {Training loss}=3.842442269276765e+30\n",
      " 62%|█████████████████████████████████████████████████▎                              | 370/600 [22:08<13:47,  3.60s/it]2020-12-08 20:59:29,586 [nnabla][INFO]: iter=444999 {Training loss}=4.6732620316970833e+30\n",
      " 62%|█████████████████████████████████████████████████▊                              | 374/600 [22:23<13:34,  3.60s/it]2020-12-08 20:59:44,573 [nnabla][INFO]: iter=449999 {Training loss}=1.350062166656994e+30\n",
      " 63%|██████████████████████████████████████████████████▍                             | 378/600 [22:37<13:16,  3.59s/it]2020-12-08 20:59:59,546 [nnabla][INFO]: iter=454999 {Training loss}=2.077043965884608e+30\n",
      " 64%|███████████████████████████████████████████████████                             | 383/600 [22:55<12:57,  3.58s/it]2020-12-08 21:00:14,441 [nnabla][INFO]: iter=459999 {Training loss}=8.61940898009494e+30\n",
      " 64%|███████████████████████████████████████████████████▌                            | 387/600 [23:09<12:47,  3.60s/it]2020-12-08 21:00:29,434 [nnabla][INFO]: iter=464999 {Training loss}=1.3500890652564804e+30\n",
      " 65%|████████████████████████████████████████████████████▏                           | 391/600 [23:24<12:35,  3.62s/it]2020-12-08 21:00:44,559 [nnabla][INFO]: iter=469999 {Training loss}=8.723237574112542e+30\n",
      " 66%|████████████████████████████████████████████████████▋                           | 395/600 [23:38<12:14,  3.59s/it]2020-12-08 21:00:59,493 [nnabla][INFO]: iter=474999 {Training loss}=8.827110898385471e+30\n",
      " 66%|█████████████████████████████████████████████████████▏                          | 399/600 [23:53<11:59,  3.58s/it]2020-12-08 21:01:14,443 [nnabla][INFO]: iter=479999 {Training loss}=9.657822461944934e+30\n",
      " 67%|█████████████████████████████████████████████████████▋                          | 403/600 [24:07<11:50,  3.60s/it]2020-12-08 21:01:29,466 [nnabla][INFO]: iter=484999 {Training loss}=2.1808755822167595e+30\n",
      " 68%|██████████████████████████████████████████████████████▎                         | 407/600 [24:21<11:31,  3.58s/it]2020-12-08 21:01:44,350 [nnabla][INFO]: iter=489999 {Training loss}=7.269403935182923e+30\n",
      " 69%|██████████████████████████████████████████████████████▉                         | 412/600 [24:39<11:15,  3.59s/it]2020-12-08 21:01:59,354 [nnabla][INFO]: iter=494999 {Training loss}=3.0552114984295395e+29\n",
      " 69%|███████████████████████████████████████████████████████▍                        | 416/600 [24:54<11:02,  3.60s/it]2020-12-08 21:02:14,320 [nnabla][INFO]: iter=499999 {Training loss}=-3.523647113934442e+28\n",
      " 70%|████████████████████████████████████████████████████████                        | 420/600 [25:08<10:41,  3.56s/it]2020-12-08 21:02:29,118 [nnabla][INFO]: iter=504999 {Training loss}=-4.090137002616354e+28\n",
      " 71%|████████████████████████████████████████████████████████▌                       | 424/600 [25:22<10:29,  3.58s/it]2020-12-08 21:02:44,052 [nnabla][INFO]: iter=509999 {Training loss}=-5.235477574249088e+27\n",
      " 71%|█████████████████████████████████████████████████████████                       | 428/600 [25:37<10:16,  3.59s/it]2020-12-08 21:02:59,100 [nnabla][INFO]: iter=514999 {Training loss}=-6.040448032848375e+27\n",
      " 72%|█████████████████████████████████████████████████████████▌                      | 432/600 [25:51<10:03,  3.59s/it]2020-12-08 21:03:14,009 [nnabla][INFO]: iter=519999 {Training loss}=-3.1585581277239e+28\n",
      " 73%|██████████████████████████████████████████████████████████▎                     | 437/600 [26:09<09:43,  3.58s/it]2020-12-08 21:03:28,995 [nnabla][INFO]: iter=524999 {Training loss}=-7.284225970194308e+30\n",
      " 74%|██████████████████████████████████████████████████████████▊                     | 441/600 [26:24<09:32,  3.60s/it]2020-12-08 21:03:43,999 [nnabla][INFO]: iter=529999 {Training loss}=-1.1029043474970414e+29\n",
      " 74%|███████████████████████████████████████████████████████████▎                    | 445/600 [26:38<09:15,  3.58s/it]2020-12-08 21:03:58,872 [nnabla][INFO]: iter=534999 {Training loss}=-7.640881645725312e+27\n",
      " 75%|███████████████████████████████████████████████████████████▊                    | 449/600 [26:52<08:55,  3.55s/it]2020-12-08 21:04:13,657 [nnabla][INFO]: iter=539999 {Training loss}=-1.5351484310203712e+28\n",
      " 76%|████████████████████████████████████████████████████████████▍                   | 453/600 [27:06<08:42,  3.55s/it]2020-12-08 21:04:28,394 [nnabla][INFO]: iter=544999 {Training loss}=-3.1091192007780896e+28\n",
      " 76%|████████████████████████████████████████████████████████████▉                   | 457/600 [27:20<08:29,  3.56s/it]2020-12-08 21:04:43,266 [nnabla][INFO]: iter=549999 {Training loss}=-9.635877832683164e+27\n",
      " 77%|█████████████████████████████████████████████████████████████▌                  | 462/600 [27:39<08:16,  3.60s/it]2020-12-08 21:04:58,276 [nnabla][INFO]: iter=554999 {Training loss}=-3.5206363691832887e+28\n",
      " 78%|██████████████████████████████████████████████████████████████▏                 | 466/600 [27:53<08:00,  3.59s/it]2020-12-08 21:05:13,189 [nnabla][INFO]: iter=559999 {Training loss}=-3.9931613769736134e+27\n",
      " 78%|██████████████████████████████████████████████████████████████▋                 | 470/600 [28:07<07:41,  3.55s/it]2020-12-08 21:05:27,918 [nnabla][INFO]: iter=564999 {Training loss}=-6.615367510985036e+29\n",
      " 79%|███████████████████████████████████████████████████████████████▏                | 474/600 [28:21<07:28,  3.56s/it]2020-12-08 21:05:42,788 [nnabla][INFO]: iter=569999 {Training loss}=-1.036906536926378e+29\n",
      " 80%|███████████████████████████████████████████████████████████████▋                | 478/600 [28:36<07:14,  3.56s/it]2020-12-08 21:05:57,653 [nnabla][INFO]: iter=574999 {Training loss}=8.715097829345412e+25\n",
      " 80%|████████████████████████████████████████████████████████████████▎               | 482/600 [28:50<07:06,  3.61s/it]2020-12-08 21:06:12,902 [nnabla][INFO]: iter=579999 {Training loss}=9.348923344032054e+25\n",
      " 81%|████████████████████████████████████████████████████████████████▉               | 487/600 [29:09<06:55,  3.68s/it]2020-12-08 21:06:28,394 [nnabla][INFO]: iter=584999 {Training loss}=2.155006012064819e+26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|█████████████████████████████████████████████████████████████████▍              | 491/600 [29:23<06:33,  3.61s/it]2020-12-08 21:06:43,408 [nnabla][INFO]: iter=589999 {Training loss}=5.7044277875053685e+25\n",
      " 82%|██████████████████████████████████████████████████████████████████              | 495/600 [29:38<06:19,  3.61s/it]2020-12-08 21:06:58,482 [nnabla][INFO]: iter=594999 {Training loss}=7.60590340922809e+25\n",
      " 83%|██████████████████████████████████████████████████████████████████▌             | 499/600 [29:52<06:03,  3.60s/it]2020-12-08 21:07:13,485 [nnabla][INFO]: iter=599999 {Training loss}=7.922816627740013e+25\n",
      " 84%|███████████████████████████████████████████████████████████████████             | 503/600 [30:07<05:50,  3.62s/it]2020-12-08 21:07:28,546 [nnabla][INFO]: iter=604999 {Training loss}=2.012395340435615e+26\n",
      " 84%|███████████████████████████████████████████████████████████████████▌            | 507/600 [30:21<05:35,  3.61s/it]2020-12-08 21:07:43,626 [nnabla][INFO]: iter=609999 {Training loss}=7.13053450379741e+25\n",
      " 85%|████████████████████████████████████████████████████████████████████▎           | 512/600 [30:39<05:16,  3.60s/it]2020-12-08 21:07:58,624 [nnabla][INFO]: iter=614999 {Training loss}=7.165455053046269e+30\n",
      " 86%|████████████████████████████████████████████████████████████████████▊           | 516/600 [30:54<05:08,  3.67s/it]2020-12-08 21:08:13,984 [nnabla][INFO]: iter=619999 {Training loss}=7.764360018484052e+25\n",
      " 87%|█████████████████████████████████████████████████████████████████████▎          | 520/600 [31:09<05:01,  3.77s/it]2020-12-08 21:08:29,779 [nnabla][INFO]: iter=624999 {Training loss}=5.229058882074688e+25\n",
      " 87%|█████████████████████████████████████████████████████████████████████▊          | 524/600 [31:24<04:43,  3.73s/it]2020-12-08 21:08:45,385 [nnabla][INFO]: iter=629999 {Training loss}=-5.229058882074688e+25\n",
      " 88%|██████████████████████████████████████████████████████████████████████▍         | 528/600 [31:38<04:22,  3.64s/it]2020-12-08 21:09:00,482 [nnabla][INFO]: iter=634999 {Training loss}=8.873554438601374e+25\n",
      " 89%|██████████████████████████████████████████████████████████████████████▉         | 532/600 [31:53<04:08,  3.65s/it]2020-12-08 21:09:15,582 [nnabla][INFO]: iter=639999 {Training loss}=6.655165598366729e+25\n",
      " 90%|███████████████████████████████████████████████████████████████████████▌        | 537/600 [32:11<03:45,  3.59s/it]2020-12-08 21:09:30,464 [nnabla][INFO]: iter=644999 {Training loss}=5.862883935592728e+25\n",
      " 90%|████████████████████████████████████████████████████████████████████████▏       | 541/600 [32:25<03:30,  3.57s/it]2020-12-08 21:09:45,300 [nnabla][INFO]: iter=649999 {Training loss}=9.190466734776093e+25\n",
      " 91%|████████████████████████████████████████████████████████████████████████▋       | 545/600 [32:40<03:17,  3.59s/it]2020-12-08 21:10:00,277 [nnabla][INFO]: iter=654999 {Training loss}=7.60590340922809e+25\n",
      " 92%|█████████████████████████████████████████████████████████████████████████▏      | 549/600 [32:54<03:05,  3.64s/it]2020-12-08 21:10:15,471 [nnabla][INFO]: iter=659999 {Training loss}=7.764360018484052e+25\n",
      " 92%|█████████████████████████████████████████████████████████████████████████▋      | 553/600 [33:09<02:48,  3.59s/it]2020-12-08 21:10:30,365 [nnabla][INFO]: iter=664999 {Training loss}=4.595233367388046e+25\n",
      " 93%|██████████████████████████████████████████████████████████████████████████▎     | 557/600 [33:23<02:35,  3.63s/it]2020-12-08 21:10:45,508 [nnabla][INFO]: iter=669999 {Training loss}=8.08127231465877e+25\n",
      " 94%|██████████████████████████████████████████████████████████████████████████▉     | 562/600 [33:41<02:18,  3.64s/it]2020-12-08 21:11:00,684 [nnabla][INFO]: iter=674999 {Training loss}=5.387515030162048e+25\n",
      " 94%|███████████████████████████████████████████████████████████████████████████▍    | 566/600 [33:56<02:02,  3.61s/it]2020-12-08 21:11:15,711 [nnabla][INFO]: iter=679999 {Training loss}=3.802951704614045e+25\n",
      " 95%|████████████████████████████████████████████████████████████████████████████    | 570/600 [34:10<01:48,  3.61s/it]2020-12-08 21:11:30,759 [nnabla][INFO]: iter=684999 {Training loss}=6.813621746454089e+25\n",
      " 96%|████████████████████████████████████████████████████████████████████████████▌   | 574/600 [34:24<01:32,  3.57s/it]2020-12-08 21:11:45,511 [nnabla][INFO]: iter=689999 {Training loss}=6.97207835571005e+25\n",
      " 96%|█████████████████████████████████████████████████████████████████████████████   | 578/600 [34:38<01:17,  3.54s/it]2020-12-08 21:12:00,270 [nnabla][INFO]: iter=694999 {Training loss}=9.190466734776093e+25\n",
      " 97%|█████████████████████████████████████████████████████████████████████████████▌  | 582/600 [34:53<01:04,  3.60s/it]2020-12-08 21:12:15,294 [nnabla][INFO]: iter=699999 {Training loss}=5.7044277875053685e+25\n",
      " 98%|██████████████████████████████████████████████████████████████████████████████▎ | 587/600 [35:11<00:47,  3.64s/it]2020-12-08 21:12:30,466 [nnabla][INFO]: iter=704999 {Training loss}=5.229058882074688e+25\n",
      " 98%|██████████████████████████████████████████████████████████████████████████████▊ | 591/600 [35:25<00:32,  3.61s/it]2020-12-08 21:12:45,451 [nnabla][INFO]: iter=709999 {Training loss}=5.387515030162048e+25\n",
      " 99%|███████████████████████████████████████████████████████████████████████████████▎| 595/600 [35:40<00:18,  3.64s/it]2020-12-08 21:13:00,654 [nnabla][INFO]: iter=714999 {Training loss}=5.229058882074688e+25\n",
      "100%|███████████████████████████████████████████████████████████████████████████████▊| 599/600 [35:54<00:03,  3.61s/it]2020-12-08 21:13:15,630 [nnabla][INFO]: iter=719999 {Training loss}=7.922816627740013e+25\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 600/600 [35:58<00:00,  3.60s/it]\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = np.repeat(np.arange(n_speakers), n_utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth.reshape((1,n_speakers*n_utterances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch, test_labels = generate_batch(dataset, labels, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_var = nn.Variable((13, batch_size, 141))\n",
    "test_batch_var.d = test_batch\n",
    "inferences = encoder_network(test_batch_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0000000e+00, 0.0000000e+00, 1.4012985e-45, 1.4012985e-45,\n",
       "       2.8025969e-45, 2.8025969e-45, 4.2038954e-45, 4.2038954e-45,\n",
       "       5.6051939e-45, 5.6051939e-45, 7.0064923e-45, 7.0064923e-45,\n",
       "       8.4077908e-45, 8.4077908e-45, 9.8090893e-45, 9.8090893e-45,\n",
       "       1.1210388e-44, 1.1210388e-44, 1.2611686e-44, 1.2611686e-44,\n",
       "       3.6327112e-01, 3.5967740e-01, 4.4778126e-01, 3.5888508e-01,\n",
       "       2.3095544e-01, 1.6854915e-01, 1.7704542e-01, 1.5365802e-01,\n",
       "       1.0272297e-01, 2.9998496e-01, 4.0747026e-01, 3.3507711e-01,\n",
       "       2.8603339e-01, 3.2031783e-01, 3.7857255e-01, 3.5380346e-01,\n",
       "       3.0281183e-01, 2.8828996e-01, 2.7170804e-01, 2.4931201e-01,\n",
       "       3.1725082e-01, 3.7840536e-01, 4.2192289e-01, 4.0436459e-01,\n",
       "       4.4591850e-01, 4.7905219e-01, 4.1939542e-01, 3.5325915e-01,\n",
       "       3.9275444e-01, 4.2455089e-01, 4.0084928e-01, 3.2496920e-01,\n",
       "       1.9948927e-01, 1.6811277e-01, 1.1786614e-01, 1.1500115e-01,\n",
       "       1.5209943e-01, 1.9923288e-01, 2.4022564e-01, 2.2183426e-01,\n",
       "       1.8054536e-01, 1.5183021e-01, 1.6008168e-01, 1.3322078e-01,\n",
       "       1.0925930e-01, 7.4726142e-02, 5.9262589e-02, 1.1047809e-01,\n",
       "       3.1376347e-01, 3.8387004e-01, 3.8509354e-01, 3.6029434e-01,\n",
       "       3.5478470e-01, 3.5118458e-01, 3.7267229e-01, 3.5830075e-01,\n",
       "       2.6759323e-01, 2.0884189e-01, 1.7168628e-01, 1.5694945e-01,\n",
       "       1.2371122e-01, 1.1211058e-01, 1.2253840e-01, 3.5297361e-01,\n",
       "       3.6668402e-01, 3.4887749e-01, 3.6556244e-01, 3.4018889e-01,\n",
       "       3.4910578e-01, 3.8453788e-01, 4.4348192e-01, 4.6483892e-01,\n",
       "       4.4577664e-01, 4.7271568e-01, 4.5824033e-01, 4.3914068e-01,\n",
       "       4.7955588e-01, 4.2248911e-01, 4.3135643e-01, 3.9716685e-01,\n",
       "       3.9530662e-01, 3.3147869e-01, 2.9524294e-01, 3.8601011e-01,\n",
       "       3.7861571e-01, 3.1528452e-01, 2.4807365e-01, 2.0296274e-01,\n",
       "       1.8723862e-01, 2.6461500e-01, 2.9611659e-01, 2.9551899e-01,\n",
       "       3.3760712e-01, 3.7292555e-01, 3.8068280e-01, 3.0587041e-01,\n",
       "       2.9413345e-01, 3.1858647e-01, 3.0484688e-01, 2.8044945e-01,\n",
       "       2.9366937e-01, 3.0924150e-01, 2.7457961e-01, 2.2006474e-01,\n",
       "       1.7387733e-01, 8.9400306e-02, 1.2728728e-01, 1.3961121e-01,\n",
       "       0.0000000e+00, 0.0000000e+00, 1.4012985e-45, 1.4012985e-45,\n",
       "       2.8025969e-45, 2.8025969e-45, 4.2038954e-45, 4.2038954e-45,\n",
       "       5.6051939e-45, 5.6051939e-45, 7.0064923e-45, 7.0064923e-45,\n",
       "       8.4077908e-45, 8.4077908e-45, 9.8090893e-45, 9.8090893e-45,\n",
       "       1.1210388e-44, 1.1210388e-44, 1.2611686e-44, 1.2611686e-44,\n",
       "       2.2897328e-01, 2.2008874e-01, 2.6820353e-01, 2.8227234e-01,\n",
       "       2.5077465e-01, 2.5356996e-01, 2.5292709e-01, 1.5404104e-01,\n",
       "       9.8688595e-02, 1.7166376e-01, 2.6993093e-01, 3.7998271e-01,\n",
       "       3.5490528e-01, 3.6664644e-01, 3.8289902e-01, 4.1114810e-01,\n",
       "       4.9596342e-01, 5.5348140e-01, 5.3525931e-01, 4.6710309e-01,\n",
       "       4.1766179e-01, 3.5873976e-01, 2.9822737e-01, 2.1997581e-01,\n",
       "       2.0122729e-01, 2.4706493e-01, 2.4657647e-01, 2.4036652e-01,\n",
       "       2.7717608e-01, 2.7801946e-01, 4.2870498e-01, 4.3306491e-01,\n",
       "       4.4446608e-01, 4.9854764e-01, 4.6902364e-01, 3.1358194e-01,\n",
       "       3.2073027e-01, 3.6129248e-01, 3.9489275e-01, 2.8901592e-01,\n",
       "       2.3030980e-01, 2.4419063e-01, 3.2023928e-01, 3.7907779e-01,\n",
       "       3.5940427e-01, 3.3957577e-01, 3.3543953e-01, 3.5471204e-01,\n",
       "       3.4737208e-01, 2.9603657e-01, 3.2561970e-01, 3.5523778e-01,\n",
       "       3.5263714e-01, 2.9290003e-01, 2.2899711e-01, 1.8370101e-01,\n",
       "       1.9277339e-01, 1.8080842e-01, 1.5837462e-01, 9.6213140e-02,\n",
       "       7.4301451e-02, 1.0478330e-01, 1.0070318e-01, 1.3581206e-01,\n",
       "       1.2457275e-01, 1.5502135e-01, 1.6742274e-01, 1.8087825e-01,\n",
       "       1.4477974e-01, 1.3496558e-01, 2.0529637e-01, 2.2920804e-01,\n",
       "       2.2590862e-01, 2.1150036e-01, 2.2258869e-01, 2.1727993e-01,\n",
       "       1.9335319e-01, 1.5386677e-01, 2.0176761e-01, 2.6729190e-01,\n",
       "       3.3427069e-01, 3.3588326e-01, 3.0250028e-01, 2.1473929e-01,\n",
       "       2.0020944e-01, 3.2873601e-01, 3.0280361e-01, 3.4217694e-01,\n",
       "       3.2041529e-01, 2.7916607e-01, 3.1316397e-01, 3.4543243e-01,\n",
       "       4.1282699e-01, 4.8723048e-01, 4.7087827e-01, 4.5504230e-01,\n",
       "       4.1805592e-01, 3.7440988e-01, 4.0263838e-01, 3.5888201e-01,\n",
       "       3.7516057e-01, 4.0423459e-01, 3.3155054e-01, 2.9856193e-01,\n",
       "       3.6518928e-01, 3.8736546e-01, 3.8558352e-01, 3.4986222e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inferences.d[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 19 nearest neighbors...\n",
      "[t-SNE] Indexed 20 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 20 samples in 0.390s...\n",
      "[t-SNE] Computed conditional probabilities for sample 20 / 20\n",
      "[t-SNE] Mean sigma: 1125899906842624.000000\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 48.074280\n",
      "[t-SNE] KL divergence after 300 iterations: 0.585815\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(inferences.d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LJ001',\n",
       " 'LJ001',\n",
       " 'LJ002',\n",
       " 'LJ002',\n",
       " 'LJ003',\n",
       " 'LJ003',\n",
       " 'LJ004',\n",
       " 'LJ004',\n",
       " 'LJ005',\n",
       " 'LJ005',\n",
       " 'LJ006',\n",
       " 'LJ006',\n",
       " 'LJ007',\n",
       " 'LJ007',\n",
       " 'LJ008',\n",
       " 'LJ008',\n",
       " 'LJ009',\n",
       " 'LJ009',\n",
       " 'LJ010',\n",
       " 'LJ010']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.107192</td>\n",
       "      <td>-79.457039</td>\n",
       "      <td>LJ001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-48.487328</td>\n",
       "      <td>-48.236965</td>\n",
       "      <td>LJ001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.088055</td>\n",
       "      <td>61.684921</td>\n",
       "      <td>LJ002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-22.299587</td>\n",
       "      <td>118.799965</td>\n",
       "      <td>LJ002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-27.051081</td>\n",
       "      <td>-137.276535</td>\n",
       "      <td>LJ003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>113.547966</td>\n",
       "      <td>70.017235</td>\n",
       "      <td>LJ003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-57.199474</td>\n",
       "      <td>-214.560883</td>\n",
       "      <td>LJ004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-134.935043</td>\n",
       "      <td>-197.974060</td>\n",
       "      <td>LJ004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-171.150848</td>\n",
       "      <td>-21.534975</td>\n",
       "      <td>LJ005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-70.680634</td>\n",
       "      <td>45.647087</td>\n",
       "      <td>LJ005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-103.778198</td>\n",
       "      <td>-20.294849</td>\n",
       "      <td>LJ006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>114.284462</td>\n",
       "      <td>0.139255</td>\n",
       "      <td>LJ006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-9.522257</td>\n",
       "      <td>19.359596</td>\n",
       "      <td>LJ007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>98.714035</td>\n",
       "      <td>-108.183609</td>\n",
       "      <td>LJ007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>218.344330</td>\n",
       "      <td>167.373489</td>\n",
       "      <td>LJ008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-125.789482</td>\n",
       "      <td>90.279480</td>\n",
       "      <td>LJ008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>56.870796</td>\n",
       "      <td>-37.824062</td>\n",
       "      <td>LJ009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>88.775146</td>\n",
       "      <td>141.777039</td>\n",
       "      <td>LJ009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-217.821899</td>\n",
       "      <td>-131.858109</td>\n",
       "      <td>LJ010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>247.595169</td>\n",
       "      <td>-1.847961</td>\n",
       "      <td>LJ010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             x           y     id\n",
       "0     7.107192  -79.457039  LJ001\n",
       "1   -48.487328  -48.236965  LJ001\n",
       "2    40.088055   61.684921  LJ002\n",
       "3   -22.299587  118.799965  LJ002\n",
       "4   -27.051081 -137.276535  LJ003\n",
       "5   113.547966   70.017235  LJ003\n",
       "6   -57.199474 -214.560883  LJ004\n",
       "7  -134.935043 -197.974060  LJ004\n",
       "8  -171.150848  -21.534975  LJ005\n",
       "9   -70.680634   45.647087  LJ005\n",
       "10 -103.778198  -20.294849  LJ006\n",
       "11  114.284462    0.139255  LJ006\n",
       "12   -9.522257   19.359596  LJ007\n",
       "13   98.714035 -108.183609  LJ007\n",
       "14  218.344330  167.373489  LJ008\n",
       "15 -125.789482   90.279480  LJ008\n",
       "16   56.870796  -37.824062  LJ009\n",
       "17   88.775146  141.777039  LJ009\n",
       "18 -217.821899 -131.858109  LJ010\n",
       "19  247.595169   -1.847961  LJ010"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(tsne_results)\n",
    "df.insert(2,\"ID\",list(test_labels))\n",
    "df.columns = [\"x\",\"y\",\"id\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1818ada7d30>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = tsne_results\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.scatterplot(\n",
    "    x=\"x\", y=\"y\",\n",
    "    hue=\"id\",\n",
    "    palette=sns.color_palette(\"hls\", 10),\n",
    "    data= df,\n",
    "    legend=\"full\",\n",
    "    alpha=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = inferences.d[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  0.00000000e+00,  1.40129846e-45,  1.40129846e-45,\n",
       "        2.80259693e-45,  2.80259693e-45,  4.20389539e-45,  4.20389539e-45,\n",
       "        5.60519386e-45,  5.60519386e-45,  7.00649232e-45,  7.00649232e-45,\n",
       "        8.40779079e-45,  8.40779079e-45,  9.80908925e-45,  9.80908925e-45,\n",
       "        1.12103877e-44,  1.12103877e-44,  1.26116862e-44,  1.26116862e-44,\n",
       "        2.43608996e-01,  2.66062558e-01,  3.06599736e-01,  2.83012867e-01,\n",
       "        2.68361866e-01,  2.65376657e-01,  1.53422967e-01,  1.17901772e-01,\n",
       "       -8.04981887e-02, -7.36958310e-02, -1.84764061e-02, -6.90907706e-03,\n",
       "       -2.11369824e-02,  5.33500947e-02,  8.54951516e-02,  5.46493046e-02,\n",
       "        6.05032556e-02,  1.41173169e-01,  1.33467585e-01,  1.26213968e-01,\n",
       "        1.33231267e-01,  1.38954863e-01,  1.30715370e-01,  6.57021180e-02,\n",
       "        1.23611085e-01,  1.51095331e-01,  2.50834048e-01,  3.80779177e-01,\n",
       "        3.10016394e-01,  2.56708682e-01,  2.48015940e-01,  2.33545348e-01,\n",
       "        1.97218210e-01,  1.32432565e-01,  3.36963721e-02, -5.49543928e-03,\n",
       "        6.62068650e-02,  1.44173071e-01,  2.22169682e-01,  2.42036745e-01,\n",
       "        2.63831109e-01,  3.44694376e-01,  4.23233867e-01,  3.63357216e-01,\n",
       "        3.10055196e-01,  3.03609818e-01,  3.05427432e-01,  3.48037988e-01,\n",
       "        3.57210368e-01,  3.45587999e-01,  3.66204411e-01,  3.48434091e-01,\n",
       "        3.29136878e-01,  2.94430345e-01,  2.49807462e-01,  2.11305827e-01,\n",
       "        1.98993042e-01,  1.84280649e-01,  1.99972808e-01,  2.23128274e-01,\n",
       "        2.29245529e-01,  2.03912452e-01,  1.92415535e-01,  2.32964665e-01,\n",
       "        2.31547385e-01,  2.50716269e-01,  2.57290632e-01,  2.16729701e-01,\n",
       "        1.83807641e-01,  2.06558287e-01,  2.23910034e-01,  2.86334544e-01,\n",
       "        2.97245383e-01,  3.32281709e-01,  3.16943645e-01,  3.13495070e-01,\n",
       "        3.44756305e-01,  3.34627599e-01,  3.24860275e-01,  2.48287544e-01,\n",
       "        1.80803224e-01,  1.94142133e-01,  1.65345952e-01,  1.38458848e-01,\n",
       "        1.64908901e-01,  1.19193584e-01,  1.13954753e-01,  1.23522818e-01,\n",
       "        7.11868629e-02, -9.48263681e-04,  4.39757155e-03,  1.43130109e-01,\n",
       "        3.03041339e-01,  3.33411515e-01,  2.97859371e-01,  2.88787752e-01,\n",
       "        2.57805496e-01,  1.32884771e-01,  1.58842951e-01,  2.40752667e-01,\n",
       "        2.07866848e-01,  1.44060701e-01,  8.37704390e-02,  1.18876599e-01,\n",
       "        1.34339362e-01,  4.03039753e-02,  7.79910684e-02,  8.97997394e-02,\n",
       "        0.00000000e+00,  0.00000000e+00,  1.40129846e-45,  1.40129846e-45,\n",
       "        2.80259693e-45,  2.80259693e-45,  4.20389539e-45,  4.20389539e-45,\n",
       "        5.60519386e-45,  5.60519386e-45,  7.00649232e-45,  7.00649232e-45,\n",
       "        8.40779079e-45,  8.40779079e-45,  9.80908925e-45,  9.80908925e-45,\n",
       "        1.12103877e-44,  1.12103877e-44,  1.26116862e-44,  1.26116862e-44,\n",
       "        3.09306413e-01,  3.31969798e-01,  3.34830433e-01,  3.33213836e-01,\n",
       "        3.37018281e-01,  3.36040556e-01,  3.07765245e-01,  2.93427408e-01,\n",
       "        3.38666290e-01,  3.67402613e-01,  3.92208159e-01,  3.84953916e-01,\n",
       "        2.67426282e-01,  1.91398889e-01,  1.02470644e-01,  2.74034236e-02,\n",
       "        1.16828633e-02,  4.09521088e-02,  8.59757513e-02,  1.32184744e-01,\n",
       "        7.91562945e-02,  9.34355631e-02,  1.51465923e-01,  1.75159186e-01,\n",
       "        1.72233224e-01,  1.43530160e-01,  1.67332456e-01,  2.16584221e-01,\n",
       "        1.79696053e-01,  1.92224443e-01,  2.30587065e-01,  1.97837517e-01,\n",
       "        1.76582664e-01,  2.38901570e-01,  2.29813069e-01,  1.82315499e-01,\n",
       "        1.74273923e-01,  1.17443576e-01,  1.69209510e-01,  2.26015732e-01,\n",
       "        2.69452721e-01,  2.49846905e-01,  2.37868935e-01,  3.51099014e-01,\n",
       "        3.98459226e-01,  4.04650062e-01,  3.47631633e-01,  3.39383483e-01,\n",
       "        3.48574489e-01,  3.41863334e-01,  3.40581059e-01,  3.09594959e-01,\n",
       "        3.10330749e-01,  3.84013832e-01,  3.75505120e-01,  3.45545471e-01,\n",
       "        3.35725605e-01,  2.63442814e-01,  3.21122736e-01,  2.86617547e-01,\n",
       "        2.14899316e-01,  1.79826260e-01,  2.30557203e-01,  3.09322149e-01,\n",
       "        3.44736695e-01,  3.15957546e-01,  3.05847228e-01,  2.89182067e-01,\n",
       "        2.35036924e-01,  1.76067889e-01,  9.02001560e-02,  1.52612820e-01,\n",
       "        2.17927039e-01,  2.27380604e-01,  2.70756960e-01,  3.55692238e-01,\n",
       "        3.48506808e-01,  3.06106448e-01,  2.68441349e-01,  2.43778050e-01,\n",
       "        9.71154645e-02,  1.16958655e-01,  1.73879609e-01,  2.01012924e-01,\n",
       "        2.30267420e-01,  2.67456770e-01,  3.09097439e-01,  3.09917152e-01,\n",
       "        2.94432461e-01,  2.59975553e-01,  2.21430212e-01,  1.92600667e-01,\n",
       "        2.01823205e-01,  1.91969752e-01,  1.37336478e-01,  9.83029306e-02,\n",
       "        1.10696353e-01,  1.59364045e-01,  1.87952891e-01,  2.28178591e-01,\n",
       "        2.09341154e-01,  1.96348563e-01,  1.93260074e-01,  1.30151197e-01,\n",
       "        2.51240700e-01,  3.07186604e-01,  2.96784639e-01,  2.38281295e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python38364bit54af622977484619b47ea42e3ff533e7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
