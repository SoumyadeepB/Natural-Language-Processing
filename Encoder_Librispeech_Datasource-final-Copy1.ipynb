{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-24 22:08:17,108 [nnabla][INFO]: Initializing CPU extension...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import glob\n",
    "from nnabla.utils.data_source import DataSource\n",
    "from nnabla.utils.data_iterator import data_iterator\n",
    "\n",
    "\n",
    "class DatasetGenerator:\n",
    "    r\"\"\" Splits the Dataset and Labels into batches containing n_utterances for each of the n_speakers\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.dev_dir = \"./data/LibriSpeech/dev-clean\"\n",
    "        self.test_dir = \"./data/LibriSpeech/test-clean\"\n",
    "        self.label_dir = \"./data/LibriSpeech/labels/\"\n",
    "        self.mfcc_dev_dir = \"./data/LibriSpeech/mfcc-dev/\"\n",
    "        self.mfcc_test_dir = \"./data/LibriSpeech/mfcc-test/\"\n",
    "        self.n_utterances = 5  # utterances per speaker in batch\n",
    "        self.batch_size = 200\n",
    "        self.n_timesteps = 101\n",
    "        self.n_features = 40\n",
    "        self.ys = np.load(self.label_dir + 's_id.npy')\n",
    "        self.xs = []\n",
    "        mfccs = sorted(glob.glob(self.mfcc_dev_dir + '*.npy'))\n",
    "        for i in tqdm(range(len(mfccs))):\n",
    "            self.xs.append((np.load(mfccs[i])).T)\n",
    "        \n",
    "        self.n_data = len(self.xs)\n",
    "\n",
    "    # Batch_size =200 , each batch will contain 5 utterances from each of the 40 speakers\n",
    "\n",
    "    def get_features(self):\n",
    "        ys = np.load(self.label_dir + 's_id.npy')\n",
    "\n",
    "        ''' \n",
    "        Get total utterances for each speaker\n",
    "        '''\n",
    "        inputs = {k: [] for k in sorted(set(ys))}\n",
    "\n",
    "        for i, s_id in enumerate(ys):\n",
    "            inputs[s_id].append(self.xs[i])\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def prep_dataset(self):    \n",
    "        mfccs = self.get_features()\n",
    "        n_iters = self.n_data // self.batch_size\n",
    "        total_data = n_iters*self.batch_size\n",
    "        dataset = np.zeros((total_data,self.n_timesteps,self.n_features))\n",
    "        for sid in mfccs:\n",
    "            np.random.shuffle(mfccs[sid])            \n",
    "        for i in range(0, total_data, self.batch_size):            \n",
    "            for sid in mfccs:\n",
    "                for j in range(self.n_utterances):                    \n",
    "                    dataset[i+j] = mfccs[sid][j]\n",
    "                i = i+j+1            \n",
    "                mfccs[sid] = np.delete(mfccs[sid], range(self.n_utterances), axis=0)\n",
    "                mfccs[sid] = np.append(mfccs[sid], dataset[-self.n_utterances:], axis=0)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7310/7310 [01:15<00:00, 97.19it/s]\n"
     ]
    }
   ],
   "source": [
    "dg = DatasetGenerator()\n",
    "dataset = dg.prep_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import nnabla as nn\n",
    "import nnabla.parametric_functions as PF\n",
    "import nnabla.functions as F\n",
    "import nnabla.solvers as S\n",
    "from nnabla.utils.data_iterator import data_iterator_simple\n",
    "import nnabla.monitor as M\n",
    "import nnabla.initializer as I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "\n",
    "    def __init__(self,dataset):\n",
    "        '''\n",
    "        Model Params (to be moved to hparams.py)\n",
    "        '''\n",
    "        self.n_utterances = 5\n",
    "        self.n_speakers = 40\n",
    "        self.batch_size = 200\n",
    "        self.lstm_layers = 3\n",
    "        self.lstm_hidden = 256\n",
    "        self.lstm_directions = 1\n",
    "        self.affine_hidden = 256\n",
    "        self.embed_size = 256\n",
    "        self.eps = 1e-6\n",
    "        self.dataset = dataset \n",
    "#         DatasetGenerator().split_dataset()\n",
    "\n",
    "        self.sim_weight = 0.0 \n",
    "        self.sim_bias = 0.0\n",
    "        self.sim_matrix = 0.0 \n",
    "        self.eps = 1e-5\n",
    "\n",
    "    def get_batch(self,i):\n",
    "        '''\n",
    "        Divide the dataset into batches containing 'm' utterances for 'n' speakers\n",
    "        '''\n",
    "\n",
    "        batch_size = self.batch_size\n",
    "        batch_data = self.dataset[i*batch_size:(i+1)*batch_size]\n",
    "#         self.dataset = np.delete(self.dataset, range(batch_size))\n",
    "#         self.dataset = np.concatenate((self.dataset,batch_data), axis = 0)\n",
    "        # convert shape from (B,T,I)  -> (T,B,I)  ((200, 138, 13) -> (138, 200, 13))\n",
    "        batch_data = np.transpose(batch_data, (1, 0, 2))\n",
    "\n",
    "        return batch_data\n",
    "\n",
    "    def encoder(self, inputs, training=True):\n",
    "        '''\n",
    "        Speaker Encoder network\n",
    "        '''\n",
    "\n",
    "        L = self.lstm_layers\n",
    "        D = self.lstm_directions\n",
    "        B = self.batch_size\n",
    "        H = self.lstm_hidden\n",
    "        T, _, I = inputs.shape\n",
    "        h = nn.Variable(shape=(L, D, B, H), need_grad=True)\n",
    "        c = nn.Variable(shape=(L, D, B, H), need_grad=True)\n",
    "        h.d = np.zeros(h.shape)\n",
    "        c.d = np.zeros(c.shape)\n",
    "\n",
    "        y, hn, cn = PF.lstm(inputs, h, c, num_layers=L, training=training)\n",
    "        \n",
    "        out = PF.affine(hn[-1][0], self.affine_hidden)\n",
    "        out = F.relu(out)\n",
    "        out = out/(F.norm(out,axis=1).reshape((-1,1)) + self.eps)\n",
    "        \n",
    "#        print(out.d)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def similarity_matrix(self,emb):\n",
    "        N = self.n_speakers\n",
    "        M = self.n_utterances\n",
    "        P = self.embed_size\n",
    "        \n",
    "        emb_re = emb.reshape((N, M, P))\n",
    "\n",
    "        # compute the inclusion centroids\n",
    "        cen = F.mean(emb_re, axis=1) \n",
    "        cen = cen / F.norm(cen, axis=1, keepdims=True)\n",
    "        cen = F.reshape(cen, (-1, P))\n",
    "\n",
    "        # compute the exclusion centroids\n",
    "        exc = F.sum(emb_re, axis=1, keepdims=True) - emb_re\n",
    "        exc = exc / (M - 1)\n",
    "        exc = exc / F.norm(exc, axis=2, keepdims=True)\n",
    "        exc = F.reshape(exc, emb.shape)\n",
    "\n",
    "        diag = F.sum(exc * emb, axis=1, keepdims=True) # 20 x 1\n",
    "        sim = F.affine(emb,  F.transpose(cen, (1, 0))) \n",
    "\n",
    "        mask = np.concatenate([np.tile(w, (M, 1)) for w in np.eye(N)])\n",
    "        mask = nn.Variable.from_numpy_array(mask)\n",
    "        sm = (1 - mask) * sim + F.tile(diag, N) * mask\n",
    "        #sm = self.sim_weight*sm + self.sim_bias\n",
    "\n",
    "        sm = F.add_scalar(F.mul_scalar(sm, self.sim_weight.d), self.sim_bias.d)\n",
    "        \n",
    "        return sm\n",
    "\n",
    "#     def similarity_matrix(self, embeddings):\n",
    "#         \"\"\"\n",
    "#             Computes the similarity matrix according the section 2.1 of GE2E.\n",
    "#         \"\"\"\n",
    "#         N = self.n_speakers\n",
    "#         M = self.n_utterances\n",
    "#         P = self.embed_size\n",
    "#         embeddings = embeddings.reshape(\n",
    "#             (self.n_speakers, self.n_utterances, self.embed_size))\n",
    "       \n",
    "#         center = F.mean(embeddings, axis=1)             # [N,P] normalized center vectors eq.(1)\n",
    "#         center_norm = F.norm(center,2,keepdims=True)\n",
    "#         center = center/center_norm\n",
    "#         center_except = F.reshape(F.sum(embeddings, axis=1, keepdims=True) - embeddings, (N*M,P))  # [NM,P] center vectors eq.(8)\n",
    "#         # make similarity matrix eq.(9)\n",
    "                \n",
    "#         sm = np.concatenate(\n",
    "#             [np.concatenate([np.sum(center_except.d[i*M:(i+1)*M,:]*embeddings.d[j,:,:], axis=1, keepdims=True) if i==j\n",
    "#                         else np.sum(center.d[i:(i+1),:]*embeddings.d[j,:,:], axis=1, keepdims=True) for i in range(N)],\n",
    "#                        axis=1) for j in range(N)], axis=0)\n",
    "#         self.sim_matrix = nn.Variable.from_numpy_array(sm)\n",
    "#         self.sim_matrix = F.add_scalar(F.mul_scalar(self.sim_matrix, self.sim_weight.d), self.sim_bias.d)\n",
    "#         #print(self.sim_matrix.d)\n",
    "#         return nn.Variable.from_numpy_array(sm)\n",
    "        \n",
    "    def loss_fn(self, embeddings):\n",
    "        '''\n",
    "        Define a cross entropy loss between the similarity matrix and the ground truth labels\n",
    "        '''\n",
    "        sim_matrix = self.similarity_matrix(embeddings)\n",
    "        #self.sim_matrix.data.copy_from(sm.data)\n",
    "        #self.sim_matrix = self.sim_matrix.reshape(\n",
    "        #    (self.n_speakers * self.n_utterances, self.n_speakers))\n",
    "\n",
    "        # Create ground truth labels\n",
    "        ground_truth = nn.Variable.from_numpy_array(\n",
    "            np.repeat(np.arange(self.n_speakers), self.n_utterances))\n",
    "        ground_truth = ground_truth.reshape(\n",
    "            (self.n_speakers * self.n_utterances, 1))\n",
    "\n",
    "        loss = F.softmax_cross_entropy(sim_matrix, ground_truth)\n",
    "        \n",
    "        return loss\n",
    "          \n",
    "\n",
    "    def train(self):\n",
    "        nn.clear_parameters()\n",
    "        #nn.set_auto_forward(True)\n",
    "        mfcc_dim1, mfcc_dim2 = self.dataset[0].shape\n",
    "        n_batch = len(self.dataset) // self.batch_size\n",
    "        max_epochs = 600\n",
    "        loss_scale = 8\n",
    "        monitor = M.Monitor('.')\n",
    "        monitor_loss = M.MonitorSeries(\n",
    "            \"Training loss\", monitor, interval=2, verbose=True)\n",
    "        monitor_time = M.MonitorTimeElapsed(\n",
    "            \"Training time\", monitor, interval=1000, verbose=True)\n",
    "        self.sim_weight = nn.parameter.get_parameter_or_create(\n",
    "            'sim_weight', (1,), I.ConstantInitializer(10.0), need_grad=True)\n",
    "        self.sim_bias = nn.parameter.get_parameter_or_create(\n",
    "             'sim_bias', (1,), I.ConstantInitializer(-5.0), need_grad=True)\n",
    "        \n",
    "#         self.sim_matrix = nn.Variable.from_numpy_array(np.zeros((self.n_speakers * self.n_utterances, self.n_speakers)), \n",
    "#                                                        need_grad = True)\n",
    "\n",
    "        solver = S.Adam()\n",
    "        xi = nn.Variable((mfcc_dim1, self.batch_size, mfcc_dim2), need_grad=True)\n",
    "        # Get the embeddings from the encoder\n",
    "        embeddings = self.encoder(xi, training=True)\n",
    "        loss = self.loss_fn(embeddings) # Define loss\n",
    "        solver.set_parameters(nn.get_parameters())\n",
    "        print(nn.get_parameters())\n",
    "\n",
    "        for epoch in range(max_epochs):\n",
    "            \n",
    "            # Iterations per epoch\n",
    "\n",
    "            for i in range(n_batch):\n",
    "\n",
    "                # Returns current batch\n",
    "                xi.d = self.get_batch(i)\n",
    "\n",
    "                loss.forward()\n",
    "                solver.zero_grad()\n",
    "                loss.backward(clear_buffer=True)\n",
    "                solver.scale_grad(1. / loss_scale)\n",
    "                solver.update()\n",
    "\n",
    "                # monitor\n",
    "                itr = epoch * n_batch + i\n",
    "                monitor_loss.add(itr, loss.d)\n",
    "            print(f\"Epoch:{epoch} | Loss:{loss.d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('sim_weight', <Variable((1,), need_grad=True) at 0x2996100b450>), ('sim_bias', <Variable((1,), need_grad=True) at 0x29961043810>), ('lstm/weight_l0', <Variable((1, 4, 256, 296), need_grad=True) at 0x299612fa6d0>), ('lstm/weight', <Variable((2, 1, 4, 256, 512), need_grad=True) at 0x299612faa90>), ('lstm/bias', <Variable((3, 1, 4, 256), need_grad=True) at 0x299612fabd0>), ('affine/W', <Variable((256, 256), need_grad=True) at 0x299612faae0>), ('affine/b', <Variable((256,), need_grad=True) at 0x299612fac70>)])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-24 22:11:19,776 [nnabla][INFO]: iter=1 {Training loss}=3.5063488483428955\n",
      "2021-01-24 22:11:38,153 [nnabla][INFO]: iter=3 {Training loss}=3.296698570251465\n",
      "2021-01-24 22:11:57,328 [nnabla][INFO]: iter=5 {Training loss}=3.2159783840179443\n",
      "2021-01-24 22:12:16,760 [nnabla][INFO]: iter=7 {Training loss}=3.0217413902282715\n",
      "2021-01-24 22:12:36,028 [nnabla][INFO]: iter=9 {Training loss}=2.9759716987609863\n",
      "2021-01-24 22:12:55,646 [nnabla][INFO]: iter=11 {Training loss}=2.984682559967041\n",
      "2021-01-24 22:13:14,974 [nnabla][INFO]: iter=13 {Training loss}=3.074453353881836\n",
      "2021-01-24 22:13:33,942 [nnabla][INFO]: iter=15 {Training loss}=2.9013309478759766\n",
      "2021-01-24 22:13:52,806 [nnabla][INFO]: iter=17 {Training loss}=2.781064987182617\n",
      "2021-01-24 22:14:11,942 [nnabla][INFO]: iter=19 {Training loss}=2.9247536659240723\n",
      "2021-01-24 22:14:30,685 [nnabla][INFO]: iter=21 {Training loss}=2.72047758102417\n",
      "2021-01-24 22:14:49,615 [nnabla][INFO]: iter=23 {Training loss}=2.7683305740356445\n",
      "2021-01-24 22:15:08,706 [nnabla][INFO]: iter=25 {Training loss}=2.7229747772216797\n",
      "2021-01-24 22:15:27,719 [nnabla][INFO]: iter=27 {Training loss}=2.8561389446258545\n",
      "2021-01-24 22:15:46,674 [nnabla][INFO]: iter=29 {Training loss}=2.6274123191833496\n",
      "2021-01-24 22:16:05,500 [nnabla][INFO]: iter=31 {Training loss}=2.8765828609466553\n",
      "2021-01-24 22:16:24,147 [nnabla][INFO]: iter=33 {Training loss}=2.6592493057250977\n",
      "2021-01-24 22:16:42,808 [nnabla][INFO]: iter=35 {Training loss}=2.7106690406799316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 | Loss:[[ 1.1343552 ]\n",
      " [ 0.93748546]\n",
      " [ 1.2255394 ]\n",
      " [ 0.9675657 ]\n",
      " [ 0.9978937 ]\n",
      " [ 2.2459593 ]\n",
      " [ 1.6836097 ]\n",
      " [ 1.6575428 ]\n",
      " [ 1.668905  ]\n",
      " [ 1.6626154 ]\n",
      " [ 3.1590657 ]\n",
      " [ 2.4845529 ]\n",
      " [ 2.3917468 ]\n",
      " [ 4.165096  ]\n",
      " [ 4.165096  ]\n",
      " [ 1.4441309 ]\n",
      " [ 3.8939636 ]\n",
      " [ 1.4441309 ]\n",
      " [ 2.8362255 ]\n",
      " [ 1.7490937 ]\n",
      " [ 1.5424505 ]\n",
      " [ 1.4881225 ]\n",
      " [ 1.3491092 ]\n",
      " [ 1.3916821 ]\n",
      " [ 1.5216861 ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 6.0893693 ]\n",
      " [ 5.1690626 ]\n",
      " [ 5.9380436 ]\n",
      " [ 2.039061  ]\n",
      " [ 2.19873   ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.1456673 ]\n",
      " [ 1.938043  ]\n",
      " [ 1.7798697 ]\n",
      " [ 2.138361  ]\n",
      " [ 4.027737  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 1.4443799 ]\n",
      " [ 2.382032  ]\n",
      " [ 1.4868687 ]\n",
      " [ 2.0460796 ]\n",
      " [ 1.5073981 ]\n",
      " [ 2.5105753 ]\n",
      " [ 2.4046013 ]\n",
      " [ 2.4680908 ]\n",
      " [ 2.5739255 ]\n",
      " [ 3.6552904 ]\n",
      " [10.349234  ]\n",
      " [ 3.3225808 ]\n",
      " [ 3.3225808 ]\n",
      " [ 3.3225808 ]\n",
      " [ 3.3225808 ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 5.66322   ]\n",
      " [ 2.4555888 ]\n",
      " [ 1.6360465 ]\n",
      " [ 4.6027536 ]\n",
      " [ 1.9340123 ]\n",
      " [ 2.1996305 ]\n",
      " [ 1.806461  ]\n",
      " [ 1.8880723 ]\n",
      " [ 1.5875214 ]\n",
      " [ 2.0178797 ]\n",
      " [ 1.0207222 ]\n",
      " [ 1.0890942 ]\n",
      " [ 3.2234726 ]\n",
      " [ 1.169147  ]\n",
      " [ 1.1234674 ]\n",
      " [ 1.8883487 ]\n",
      " [ 3.1342864 ]\n",
      " [ 1.893018  ]\n",
      " [ 1.8214688 ]\n",
      " [ 2.1970553 ]\n",
      " [ 2.2587087 ]\n",
      " [ 2.4604836 ]\n",
      " [ 4.9899797 ]\n",
      " [ 1.2890124 ]\n",
      " [ 1.4789497 ]\n",
      " [ 2.7937717 ]\n",
      " [ 2.1534362 ]\n",
      " [ 2.8181682 ]\n",
      " [ 2.0800812 ]\n",
      " [ 1.6778544 ]\n",
      " [ 1.0359944 ]\n",
      " [ 1.5084401 ]\n",
      " [ 7.845638  ]\n",
      " [ 1.0988135 ]\n",
      " [ 1.0197043 ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.8970087 ]\n",
      " [ 2.1169739 ]\n",
      " [ 2.1093388 ]\n",
      " [ 2.147344  ]\n",
      " [ 1.8250535 ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 1.5537584 ]\n",
      " [ 1.6421357 ]\n",
      " [ 2.0387504 ]\n",
      " [ 2.2789528 ]\n",
      " [ 2.73178   ]\n",
      " [ 2.0873914 ]\n",
      " [ 2.1303701 ]\n",
      " [ 2.428173  ]\n",
      " [ 2.8585808 ]\n",
      " [ 2.152138  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 1.8086905 ]\n",
      " [ 1.8943181 ]\n",
      " [ 1.937526  ]\n",
      " [ 3.447754  ]\n",
      " [ 2.063682  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 1.4080355 ]\n",
      " [ 1.3387135 ]\n",
      " [ 1.9407327 ]\n",
      " [ 1.3666263 ]\n",
      " [ 1.3448801 ]\n",
      " [ 2.6962013 ]\n",
      " [ 4.3010406 ]\n",
      " [ 1.7993125 ]\n",
      " [ 3.2892134 ]\n",
      " [ 3.0919814 ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 2.838387  ]\n",
      " [ 3.0270362 ]\n",
      " [ 2.89657   ]\n",
      " [ 2.9344642 ]\n",
      " [ 8.181415  ]\n",
      " [ 8.181415  ]\n",
      " [ 9.060833  ]\n",
      " [ 3.2539024 ]\n",
      " [ 3.2539024 ]\n",
      " [ 3.2539024 ]\n",
      " [ 3.2539024 ]\n",
      " [ 2.3319535 ]\n",
      " [ 4.164653  ]\n",
      " [ 2.8025775 ]\n",
      " [ 5.236543  ]\n",
      " [ 5.236543  ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-24 22:17:01,334 [nnabla][INFO]: iter=37 {Training loss}=2.7550432682037354\n",
      "2021-01-24 22:17:20,302 [nnabla][INFO]: iter=39 {Training loss}=2.6989781856536865\n",
      "2021-01-24 22:17:39,074 [nnabla][INFO]: iter=41 {Training loss}=2.555396795272827\n",
      "2021-01-24 22:17:57,980 [nnabla][INFO]: iter=43 {Training loss}=2.630783796310425\n",
      "2021-01-24 22:18:17,094 [nnabla][INFO]: iter=45 {Training loss}=2.7999613285064697\n",
      "2021-01-24 22:18:36,039 [nnabla][INFO]: iter=47 {Training loss}=2.5470457077026367\n",
      "2021-01-24 22:18:54,786 [nnabla][INFO]: iter=49 {Training loss}=2.7106215953826904\n",
      "2021-01-24 22:19:13,417 [nnabla][INFO]: iter=51 {Training loss}=2.56636118888855\n",
      "2021-01-24 22:19:32,242 [nnabla][INFO]: iter=53 {Training loss}=2.3179919719696045\n",
      "2021-01-24 22:19:51,269 [nnabla][INFO]: iter=55 {Training loss}=2.56302547454834\n",
      "2021-01-24 22:20:10,227 [nnabla][INFO]: iter=57 {Training loss}=2.58589243888855\n",
      "2021-01-24 22:20:28,836 [nnabla][INFO]: iter=59 {Training loss}=2.513296604156494\n",
      "2021-01-24 22:20:47,453 [nnabla][INFO]: iter=61 {Training loss}=2.3919410705566406\n",
      "2021-01-24 22:21:06,258 [nnabla][INFO]: iter=63 {Training loss}=2.479820489883423\n",
      "2021-01-24 22:21:25,029 [nnabla][INFO]: iter=65 {Training loss}=2.5293092727661133\n",
      "2021-01-24 22:21:43,774 [nnabla][INFO]: iter=67 {Training loss}=2.6495609283447266\n",
      "2021-01-24 22:22:02,386 [nnabla][INFO]: iter=69 {Training loss}=2.507058620452881\n",
      "2021-01-24 22:22:21,116 [nnabla][INFO]: iter=71 {Training loss}=2.5935723781585693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 | Loss:[[1.3675661 ]\n",
      " [1.2616485 ]\n",
      " [1.3119102 ]\n",
      " [1.2453952 ]\n",
      " [1.2799665 ]\n",
      " [4.1200924 ]\n",
      " [1.4924916 ]\n",
      " [1.6246169 ]\n",
      " [1.9535486 ]\n",
      " [1.9175243 ]\n",
      " [3.0902555 ]\n",
      " [3.1477802 ]\n",
      " [3.679193  ]\n",
      " [6.6379538 ]\n",
      " [6.6379538 ]\n",
      " [1.9281788 ]\n",
      " [1.879652  ]\n",
      " [1.9281788 ]\n",
      " [1.7137048 ]\n",
      " [2.222343  ]\n",
      " [2.2563443 ]\n",
      " [0.55323267]\n",
      " [0.90098035]\n",
      " [0.5774286 ]\n",
      " [0.87513953]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.370552  ]\n",
      " [2.2622638 ]\n",
      " [3.4437957 ]\n",
      " [3.1152973 ]\n",
      " [1.6743511 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.1080291 ]\n",
      " [2.3940406 ]\n",
      " [1.9225305 ]\n",
      " [2.8776238 ]\n",
      " [2.1240656 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [1.9812356 ]\n",
      " [2.6819901 ]\n",
      " [1.9997952 ]\n",
      " [2.8078797 ]\n",
      " [1.9340818 ]\n",
      " [2.729036  ]\n",
      " [1.4150416 ]\n",
      " [1.5077933 ]\n",
      " [2.7922924 ]\n",
      " [5.8493276 ]\n",
      " [8.055855  ]\n",
      " [3.1603503 ]\n",
      " [3.1603503 ]\n",
      " [3.1603503 ]\n",
      " [3.1603503 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [3.4103575 ]\n",
      " [2.1522622 ]\n",
      " [1.6443652 ]\n",
      " [4.7223454 ]\n",
      " [1.4180938 ]\n",
      " [3.0300937 ]\n",
      " [2.108841  ]\n",
      " [3.0280466 ]\n",
      " [1.861107  ]\n",
      " [4.2005243 ]\n",
      " [1.3779333 ]\n",
      " [1.3187464 ]\n",
      " [1.3584341 ]\n",
      " [1.3191649 ]\n",
      " [1.2368327 ]\n",
      " [1.4250332 ]\n",
      " [1.4243366 ]\n",
      " [1.4121494 ]\n",
      " [1.3692564 ]\n",
      " [1.4147178 ]\n",
      " [2.2447028 ]\n",
      " [1.5330354 ]\n",
      " [2.539084  ]\n",
      " [1.2764432 ]\n",
      " [1.8203802 ]\n",
      " [1.9725429 ]\n",
      " [2.5340362 ]\n",
      " [3.3416145 ]\n",
      " [2.333261  ]\n",
      " [3.7750144 ]\n",
      " [1.5911497 ]\n",
      " [1.7706101 ]\n",
      " [1.7440345 ]\n",
      " [1.7651212 ]\n",
      " [1.8598711 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [1.792932  ]\n",
      " [2.7600045 ]\n",
      " [1.4864864 ]\n",
      " [1.5348867 ]\n",
      " [1.5012103 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [1.7189233 ]\n",
      " [1.7685674 ]\n",
      " [1.9414402 ]\n",
      " [2.6967564 ]\n",
      " [1.9672366 ]\n",
      " [1.9589512 ]\n",
      " [1.3300602 ]\n",
      " [1.140972  ]\n",
      " [1.4489489 ]\n",
      " [3.414587  ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [1.6489357 ]\n",
      " [1.8478308 ]\n",
      " [1.8421474 ]\n",
      " [2.321708  ]\n",
      " [2.20408   ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [1.7920436 ]\n",
      " [1.8236084 ]\n",
      " [2.7009063 ]\n",
      " [1.9589024 ]\n",
      " [2.273584  ]\n",
      " [2.2381647 ]\n",
      " [1.5508444 ]\n",
      " [3.0699773 ]\n",
      " [2.7553325 ]\n",
      " [2.2788079 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.7761855 ]\n",
      " [2.8099322 ]\n",
      " [2.4556446 ]\n",
      " [2.5302    ]\n",
      " [5.778578  ]\n",
      " [5.778578  ]\n",
      " [8.964756  ]\n",
      " [3.2352264 ]\n",
      " [3.2352264 ]\n",
      " [3.2352264 ]\n",
      " [3.2352264 ]\n",
      " [3.223421  ]\n",
      " [3.1466126 ]\n",
      " [3.094257  ]\n",
      " [6.632434  ]\n",
      " [6.632434  ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-24 22:22:40,135 [nnabla][INFO]: iter=73 {Training loss}=2.5659737586975098\n",
      "2021-01-24 22:22:58,565 [nnabla][INFO]: iter=75 {Training loss}=2.515228748321533\n",
      "2021-01-24 22:23:17,372 [nnabla][INFO]: iter=77 {Training loss}=2.5283291339874268\n",
      "2021-01-24 22:23:36,041 [nnabla][INFO]: iter=79 {Training loss}=2.498563289642334\n",
      "2021-01-24 22:23:54,819 [nnabla][INFO]: iter=81 {Training loss}=2.5928497314453125\n",
      "2021-01-24 22:24:13,749 [nnabla][INFO]: iter=83 {Training loss}=2.3167104721069336\n",
      "2021-01-24 22:24:32,394 [nnabla][INFO]: iter=85 {Training loss}=2.5422873497009277\n",
      "2021-01-24 22:24:51,036 [nnabla][INFO]: iter=87 {Training loss}=2.381408929824829\n",
      "2021-01-24 22:25:09,296 [nnabla][INFO]: iter=89 {Training loss}=2.2195353507995605\n",
      "2021-01-24 22:25:27,737 [nnabla][INFO]: iter=91 {Training loss}=2.301358461380005\n",
      "2021-01-24 22:25:46,731 [nnabla][INFO]: iter=93 {Training loss}=2.382939338684082\n",
      "2021-01-24 22:26:05,323 [nnabla][INFO]: iter=95 {Training loss}=2.276193618774414\n",
      "2021-01-24 22:26:23,296 [nnabla][INFO]: iter=97 {Training loss}=2.2511954307556152\n",
      "2021-01-24 22:26:41,503 [nnabla][INFO]: iter=99 {Training loss}=2.416646957397461\n",
      "2021-01-24 22:27:00,156 [nnabla][INFO]: iter=101 {Training loss}=2.399660110473633\n",
      "2021-01-24 22:27:18,553 [nnabla][INFO]: iter=103 {Training loss}=2.452681303024292\n",
      "2021-01-24 22:27:37,136 [nnabla][INFO]: iter=105 {Training loss}=2.356412887573242\n",
      "2021-01-24 22:27:55,672 [nnabla][INFO]: iter=107 {Training loss}=2.481227159500122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2 | Loss:[[0.8653343 ]\n",
      " [0.8752197 ]\n",
      " [0.86411005]\n",
      " [0.8676011 ]\n",
      " [0.84125173]\n",
      " [1.802418  ]\n",
      " [1.1715033 ]\n",
      " [2.086986  ]\n",
      " [0.9812367 ]\n",
      " [1.1910301 ]\n",
      " [2.814827  ]\n",
      " [2.8191915 ]\n",
      " [3.12975   ]\n",
      " [5.8628664 ]\n",
      " [5.8628664 ]\n",
      " [1.797902  ]\n",
      " [1.9000118 ]\n",
      " [1.797902  ]\n",
      " [2.471164  ]\n",
      " [1.9772964 ]\n",
      " [0.8443697 ]\n",
      " [0.64369327]\n",
      " [0.6855148 ]\n",
      " [0.68634313]\n",
      " [0.9906296 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [1.8508666 ]\n",
      " [1.6670697 ]\n",
      " [2.1891274 ]\n",
      " [1.8383013 ]\n",
      " [1.6534367 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [1.3751565 ]\n",
      " [1.8613853 ]\n",
      " [1.491255  ]\n",
      " [1.7978213 ]\n",
      " [2.1261187 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [1.4899892 ]\n",
      " [1.6289246 ]\n",
      " [1.6295025 ]\n",
      " [1.4748046 ]\n",
      " [2.3896818 ]\n",
      " [1.8603178 ]\n",
      " [1.8020362 ]\n",
      " [1.8055888 ]\n",
      " [1.7356504 ]\n",
      " [7.579136  ]\n",
      " [9.834563  ]\n",
      " [3.2458165 ]\n",
      " [3.2458165 ]\n",
      " [3.2458165 ]\n",
      " [3.2458165 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [0.62994903]\n",
      " [0.5007681 ]\n",
      " [0.42439103]\n",
      " [4.1475387 ]\n",
      " [0.7866707 ]\n",
      " [2.1816163 ]\n",
      " [2.2477531 ]\n",
      " [1.6064684 ]\n",
      " [1.2350181 ]\n",
      " [2.4640393 ]\n",
      " [0.95355856]\n",
      " [0.98365414]\n",
      " [2.6109471 ]\n",
      " [0.9219917 ]\n",
      " [0.95196974]\n",
      " [1.6055286 ]\n",
      " [1.4206371 ]\n",
      " [1.6115495 ]\n",
      " [1.4986725 ]\n",
      " [1.6835077 ]\n",
      " [1.7167721 ]\n",
      " [2.4445488 ]\n",
      " [2.2186122 ]\n",
      " [1.5388138 ]\n",
      " [1.4898521 ]\n",
      " [0.8448791 ]\n",
      " [1.8420329 ]\n",
      " [4.3914447 ]\n",
      " [1.5885329 ]\n",
      " [1.7453537 ]\n",
      " [1.3530792 ]\n",
      " [1.5338022 ]\n",
      " [3.6969604 ]\n",
      " [1.6368563 ]\n",
      " [1.5712677 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.1252322 ]\n",
      " [1.8557764 ]\n",
      " [1.8513508 ]\n",
      " [1.7370313 ]\n",
      " [1.6746837 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [0.24164897]\n",
      " [0.25220737]\n",
      " [3.5684185 ]\n",
      " [0.2474589 ]\n",
      " [5.7053456 ]\n",
      " [1.7931134 ]\n",
      " [3.3516335 ]\n",
      " [2.420895  ]\n",
      " [1.8438593 ]\n",
      " [4.0779877 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [1.5411538 ]\n",
      " [1.5599529 ]\n",
      " [1.536168  ]\n",
      " [2.0509944 ]\n",
      " [1.6655662 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [1.437402  ]\n",
      " [1.3396238 ]\n",
      " [1.7992636 ]\n",
      " [1.4967563 ]\n",
      " [1.6892642 ]\n",
      " [3.613278  ]\n",
      " [2.1166043 ]\n",
      " [3.4855652 ]\n",
      " [3.838668  ]\n",
      " [3.7439284 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [2.7698927 ]\n",
      " [3.06068   ]\n",
      " [2.6899657 ]\n",
      " [2.780838  ]\n",
      " [7.6427193 ]\n",
      " [7.6427193 ]\n",
      " [8.374525  ]\n",
      " [3.2076685 ]\n",
      " [3.2076685 ]\n",
      " [3.2076685 ]\n",
      " [3.2076685 ]\n",
      " [2.8527324 ]\n",
      " [3.487209  ]\n",
      " [2.9089417 ]\n",
      " [6.2234993 ]\n",
      " [6.2234993 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-24 22:28:14,081 [nnabla][INFO]: iter=109 {Training loss}=2.635732650756836\n",
      "2021-01-24 22:28:32,573 [nnabla][INFO]: iter=111 {Training loss}=2.5408859252929688\n",
      "2021-01-24 22:28:51,270 [nnabla][INFO]: iter=113 {Training loss}=2.3992443084716797\n",
      "2021-01-24 22:29:10,021 [nnabla][INFO]: iter=115 {Training loss}=2.5622973442077637\n",
      "2021-01-24 22:29:28,164 [nnabla][INFO]: iter=117 {Training loss}=2.5285041332244873\n",
      "2021-01-24 22:29:46,528 [nnabla][INFO]: iter=119 {Training loss}=2.2657830715179443\n",
      "2021-01-24 22:30:05,243 [nnabla][INFO]: iter=121 {Training loss}=2.4464404582977295\n",
      "2021-01-24 22:30:23,887 [nnabla][INFO]: iter=123 {Training loss}=2.367598533630371\n",
      "2021-01-24 22:30:42,508 [nnabla][INFO]: iter=125 {Training loss}=2.185542106628418\n",
      "2021-01-24 22:31:00,661 [nnabla][INFO]: iter=127 {Training loss}=2.605356454849243\n",
      "2021-01-24 22:31:19,524 [nnabla][INFO]: iter=129 {Training loss}=2.4198877811431885\n",
      "2021-01-24 22:31:38,209 [nnabla][INFO]: iter=131 {Training loss}=2.370563507080078\n",
      "2021-01-24 22:31:56,669 [nnabla][INFO]: iter=133 {Training loss}=2.3463449478149414\n",
      "2021-01-24 22:32:15,635 [nnabla][INFO]: iter=135 {Training loss}=2.3202550411224365\n",
      "2021-01-24 22:32:33,694 [nnabla][INFO]: iter=137 {Training loss}=2.229465961456299\n",
      "2021-01-24 22:32:52,710 [nnabla][INFO]: iter=139 {Training loss}=2.2990951538085938\n",
      "2021-01-24 22:33:11,329 [nnabla][INFO]: iter=141 {Training loss}=2.392159938812256\n",
      "2021-01-24 22:33:30,009 [nnabla][INFO]: iter=143 {Training loss}=2.4995298385620117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3 | Loss:[[ 0.88761175]\n",
      " [ 0.89570385]\n",
      " [ 0.94925034]\n",
      " [ 0.90813947]\n",
      " [ 0.9569048 ]\n",
      " [ 2.59198   ]\n",
      " [ 1.6084218 ]\n",
      " [ 1.675326  ]\n",
      " [ 1.5668521 ]\n",
      " [ 1.8071128 ]\n",
      " [ 3.0614471 ]\n",
      " [ 1.9301097 ]\n",
      " [ 2.3042605 ]\n",
      " [ 5.4823627 ]\n",
      " [ 5.4823627 ]\n",
      " [ 1.8148698 ]\n",
      " [ 2.3847556 ]\n",
      " [ 1.8148698 ]\n",
      " [ 2.6461978 ]\n",
      " [ 2.1070428 ]\n",
      " [ 1.4459112 ]\n",
      " [ 1.5063293 ]\n",
      " [ 1.0110682 ]\n",
      " [ 1.5687407 ]\n",
      " [ 1.3811879 ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 0.75881803]\n",
      " [ 0.7330587 ]\n",
      " [ 3.2867537 ]\n",
      " [ 2.2575693 ]\n",
      " [ 1.0176997 ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 1.6369822 ]\n",
      " [ 1.6087459 ]\n",
      " [ 1.3544265 ]\n",
      " [ 2.630979  ]\n",
      " [ 1.4892975 ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 1.5271502 ]\n",
      " [ 1.0865281 ]\n",
      " [ 0.9310953 ]\n",
      " [ 1.7833841 ]\n",
      " [ 0.956928  ]\n",
      " [ 0.73673683]\n",
      " [ 0.72676206]\n",
      " [ 0.73552537]\n",
      " [ 0.8395695 ]\n",
      " [ 8.1554365 ]\n",
      " [10.484029  ]\n",
      " [ 3.297498  ]\n",
      " [ 3.297498  ]\n",
      " [ 3.297498  ]\n",
      " [ 3.297498  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 1.3476084 ]\n",
      " [ 0.32267228]\n",
      " [ 0.36323443]\n",
      " [ 2.7161908 ]\n",
      " [ 0.41204602]\n",
      " [ 1.9197288 ]\n",
      " [ 1.6819606 ]\n",
      " [ 1.7839524 ]\n",
      " [ 1.8517368 ]\n",
      " [ 2.4612446 ]\n",
      " [ 0.92697984]\n",
      " [ 0.9021052 ]\n",
      " [ 1.195234  ]\n",
      " [ 0.909683  ]\n",
      " [ 0.9202789 ]\n",
      " [ 0.8483121 ]\n",
      " [ 1.3906544 ]\n",
      " [ 1.3395699 ]\n",
      " [ 1.7397499 ]\n",
      " [ 1.4805009 ]\n",
      " [ 2.3121643 ]\n",
      " [ 1.7575343 ]\n",
      " [ 3.2368848 ]\n",
      " [ 1.9975543 ]\n",
      " [ 1.5593759 ]\n",
      " [ 2.5284395 ]\n",
      " [ 2.1018615 ]\n",
      " [ 5.3796577 ]\n",
      " [ 1.735712  ]\n",
      " [ 2.9298584 ]\n",
      " [ 0.80992466]\n",
      " [ 0.9834338 ]\n",
      " [ 1.3458812 ]\n",
      " [ 1.1344047 ]\n",
      " [ 0.91827023]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 3.4937074 ]\n",
      " [ 4.700736  ]\n",
      " [ 1.7060643 ]\n",
      " [ 2.4318986 ]\n",
      " [ 1.7713339 ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 0.6097759 ]\n",
      " [ 0.48918545]\n",
      " [ 1.0184493 ]\n",
      " [ 0.5542701 ]\n",
      " [ 0.98582935]\n",
      " [ 2.9417875 ]\n",
      " [ 2.8870149 ]\n",
      " [ 3.2751455 ]\n",
      " [ 3.4118853 ]\n",
      " [ 8.975735  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 1.1841785 ]\n",
      " [ 1.6994143 ]\n",
      " [ 1.4112625 ]\n",
      " [ 5.75277   ]\n",
      " [ 3.3050277 ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 0.8849996 ]\n",
      " [ 0.844771  ]\n",
      " [ 2.426953  ]\n",
      " [ 1.6455721 ]\n",
      " [ 1.9351907 ]\n",
      " [ 2.0023713 ]\n",
      " [ 1.6983601 ]\n",
      " [ 2.0915816 ]\n",
      " [ 2.810909  ]\n",
      " [ 2.5418396 ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 2.816036  ]\n",
      " [ 1.9787724 ]\n",
      " [ 2.6995254 ]\n",
      " [ 1.9846785 ]\n",
      " [ 6.3931913 ]\n",
      " [ 6.3931913 ]\n",
      " [ 9.170472  ]\n",
      " [ 3.3106787 ]\n",
      " [ 3.3106787 ]\n",
      " [ 3.3106787 ]\n",
      " [ 3.3106787 ]\n",
      " [ 3.1466787 ]\n",
      " [ 5.425344  ]\n",
      " [ 3.184843  ]\n",
      " [ 6.357651  ]\n",
      " [ 6.357651  ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-24 22:33:48,716 [nnabla][INFO]: iter=145 {Training loss}=2.436481237411499\n",
      "2021-01-24 22:34:06,961 [nnabla][INFO]: iter=147 {Training loss}=2.303014039993286\n",
      "2021-01-24 22:34:25,654 [nnabla][INFO]: iter=149 {Training loss}=2.2492308616638184\n",
      "2021-01-24 22:34:44,523 [nnabla][INFO]: iter=151 {Training loss}=2.192913293838501\n",
      "2021-01-24 22:35:03,041 [nnabla][INFO]: iter=153 {Training loss}=2.38405179977417\n",
      "2021-01-24 22:35:21,562 [nnabla][INFO]: iter=155 {Training loss}=2.2618868350982666\n",
      "2021-01-24 22:35:39,995 [nnabla][INFO]: iter=157 {Training loss}=2.4260270595550537\n",
      "2021-01-24 22:35:58,554 [nnabla][INFO]: iter=159 {Training loss}=2.363929033279419\n",
      "2021-01-24 22:36:17,256 [nnabla][INFO]: iter=161 {Training loss}=2.1758882999420166\n",
      "2021-01-24 22:36:35,937 [nnabla][INFO]: iter=163 {Training loss}=2.4161951541900635\n",
      "2021-01-24 22:36:54,400 [nnabla][INFO]: iter=165 {Training loss}=2.340636968612671\n",
      "2021-01-24 22:37:12,724 [nnabla][INFO]: iter=167 {Training loss}=2.270401954650879\n",
      "2021-01-24 22:37:31,615 [nnabla][INFO]: iter=169 {Training loss}=2.084566593170166\n",
      "2021-01-24 22:37:50,033 [nnabla][INFO]: iter=171 {Training loss}=2.392613172531128\n",
      "2021-01-24 22:38:08,854 [nnabla][INFO]: iter=173 {Training loss}=2.330256462097168\n",
      "2021-01-24 22:38:27,435 [nnabla][INFO]: iter=175 {Training loss}=2.473297119140625\n",
      "2021-01-24 22:38:46,099 [nnabla][INFO]: iter=177 {Training loss}=2.2719082832336426\n",
      "2021-01-24 22:39:04,953 [nnabla][INFO]: iter=179 {Training loss}=2.4043447971343994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4 | Loss:[[ 0.8723094 ]\n",
      " [ 0.8377958 ]\n",
      " [ 0.8742547 ]\n",
      " [ 0.82674456]\n",
      " [ 0.8326591 ]\n",
      " [ 2.4495735 ]\n",
      " [ 1.906151  ]\n",
      " [ 1.4306489 ]\n",
      " [ 1.2468878 ]\n",
      " [ 1.8501413 ]\n",
      " [ 2.4286394 ]\n",
      " [ 2.438681  ]\n",
      " [ 2.8337154 ]\n",
      " [ 5.5220027 ]\n",
      " [ 5.5220027 ]\n",
      " [ 1.1589084 ]\n",
      " [ 2.998796  ]\n",
      " [ 1.1589084 ]\n",
      " [ 1.6258211 ]\n",
      " [ 1.4230847 ]\n",
      " [ 1.4043038 ]\n",
      " [ 1.159064  ]\n",
      " [ 1.0841714 ]\n",
      " [ 1.7054315 ]\n",
      " [ 2.072805  ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 1.6783302 ]\n",
      " [ 1.7019978 ]\n",
      " [ 4.0175643 ]\n",
      " [ 2.1938043 ]\n",
      " [ 1.5451913 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.787196  ]\n",
      " [ 2.921031  ]\n",
      " [ 2.390805  ]\n",
      " [ 2.0943289 ]\n",
      " [ 2.6171582 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 1.0008461 ]\n",
      " [ 1.0251471 ]\n",
      " [ 1.1265053 ]\n",
      " [ 1.6564138 ]\n",
      " [ 1.7643461 ]\n",
      " [ 1.6270728 ]\n",
      " [ 0.87007266]\n",
      " [ 0.8486467 ]\n",
      " [ 1.4851578 ]\n",
      " [ 7.018913  ]\n",
      " [ 9.262818  ]\n",
      " [ 3.2392848 ]\n",
      " [ 3.2392848 ]\n",
      " [ 3.2392848 ]\n",
      " [ 3.2392848 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 0.2481387 ]\n",
      " [ 0.48693922]\n",
      " [ 1.4168714 ]\n",
      " [ 5.428379  ]\n",
      " [ 0.25604334]\n",
      " [ 3.0623593 ]\n",
      " [ 1.9130957 ]\n",
      " [ 1.6869949 ]\n",
      " [ 0.9990386 ]\n",
      " [ 2.0939817 ]\n",
      " [ 0.874255  ]\n",
      " [ 0.8697759 ]\n",
      " [ 1.1146737 ]\n",
      " [ 0.86833066]\n",
      " [ 0.90169525]\n",
      " [ 2.5712047 ]\n",
      " [ 3.722845  ]\n",
      " [ 1.6055068 ]\n",
      " [ 1.653225  ]\n",
      " [ 1.9043537 ]\n",
      " [ 1.9838617 ]\n",
      " [ 2.54947   ]\n",
      " [ 4.170576  ]\n",
      " [ 2.4013972 ]\n",
      " [ 1.7222918 ]\n",
      " [ 4.1139007 ]\n",
      " [ 3.1775517 ]\n",
      " [ 3.3958855 ]\n",
      " [ 1.2469423 ]\n",
      " [ 1.9808908 ]\n",
      " [ 0.8205898 ]\n",
      " [ 0.95024127]\n",
      " [ 0.7683363 ]\n",
      " [ 0.91681826]\n",
      " [ 0.6905769 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 1.2239617 ]\n",
      " [ 0.40777358]\n",
      " [ 0.41304457]\n",
      " [ 0.31607643]\n",
      " [ 0.37209183]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 0.11166668]\n",
      " [ 0.127798  ]\n",
      " [ 2.3383198 ]\n",
      " [ 0.38190505]\n",
      " [ 0.2409049 ]\n",
      " [ 1.5735604 ]\n",
      " [ 1.5904849 ]\n",
      " [ 1.2269195 ]\n",
      " [ 2.2965138 ]\n",
      " [ 6.6156225 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.5564396 ]\n",
      " [ 1.4234473 ]\n",
      " [ 1.437629  ]\n",
      " [ 1.4981506 ]\n",
      " [ 1.6342071 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 1.0732023 ]\n",
      " [ 1.0649669 ]\n",
      " [ 1.4683927 ]\n",
      " [ 1.2323153 ]\n",
      " [ 0.9985977 ]\n",
      " [ 2.1933074 ]\n",
      " [ 2.9570665 ]\n",
      " [ 1.7008111 ]\n",
      " [ 3.4618165 ]\n",
      " [ 4.5924563 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 2.7779768 ]\n",
      " [ 1.9548448 ]\n",
      " [ 1.8634    ]\n",
      " [ 1.507001  ]\n",
      " [ 6.241125  ]\n",
      " [ 6.241125  ]\n",
      " [10.179233  ]\n",
      " [ 3.25716   ]\n",
      " [ 3.25716   ]\n",
      " [ 3.25716   ]\n",
      " [ 3.25716   ]\n",
      " [ 2.728394  ]\n",
      " [ 3.371275  ]\n",
      " [ 2.8318262 ]\n",
      " [ 5.55662   ]\n",
      " [ 5.55662   ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-24 22:39:23,515 [nnabla][INFO]: iter=181 {Training loss}=2.396416664123535\n",
      "2021-01-24 22:39:42,297 [nnabla][INFO]: iter=183 {Training loss}=2.251049280166626\n",
      "2021-01-24 22:40:00,686 [nnabla][INFO]: iter=185 {Training loss}=2.328432083129883\n",
      "2021-01-24 22:40:19,358 [nnabla][INFO]: iter=187 {Training loss}=2.2990779876708984\n",
      "2021-01-24 22:40:37,890 [nnabla][INFO]: iter=189 {Training loss}=2.3049254417419434\n",
      "2021-01-24 22:40:56,562 [nnabla][INFO]: iter=191 {Training loss}=2.0444328784942627\n",
      "2021-01-24 22:41:15,312 [nnabla][INFO]: iter=193 {Training loss}=2.351693630218506\n",
      "2021-01-24 22:41:33,813 [nnabla][INFO]: iter=195 {Training loss}=2.3213412761688232\n",
      "2021-01-24 22:41:52,796 [nnabla][INFO]: iter=197 {Training loss}=2.0697710514068604\n",
      "2021-01-24 22:42:11,521 [nnabla][INFO]: iter=199 {Training loss}=2.2539925575256348\n",
      "2021-01-24 22:42:30,078 [nnabla][INFO]: iter=201 {Training loss}=2.2124786376953125\n",
      "2021-01-24 22:42:48,881 [nnabla][INFO]: iter=203 {Training loss}=2.337787389755249\n",
      "2021-01-24 22:43:07,260 [nnabla][INFO]: iter=205 {Training loss}=2.135040521621704\n",
      "2021-01-24 22:43:25,683 [nnabla][INFO]: iter=207 {Training loss}=2.4533791542053223\n",
      "2021-01-24 22:43:44,499 [nnabla][INFO]: iter=209 {Training loss}=2.3200998306274414\n",
      "2021-01-24 22:44:03,029 [nnabla][INFO]: iter=211 {Training loss}=2.4224393367767334\n",
      "2021-01-24 22:44:21,549 [nnabla][INFO]: iter=213 {Training loss}=2.4652178287506104\n",
      "2021-01-24 22:44:39,817 [nnabla][INFO]: iter=215 {Training loss}=2.4857771396636963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:5 | Loss:[[1.1241581 ]\n",
      " [0.7443495 ]\n",
      " [0.7500229 ]\n",
      " [0.7190602 ]\n",
      " [0.8156676 ]\n",
      " [1.9816222 ]\n",
      " [1.4329513 ]\n",
      " [1.6905818 ]\n",
      " [2.0647955 ]\n",
      " [1.8981451 ]\n",
      " [1.710107  ]\n",
      " [2.1588228 ]\n",
      " [3.448708  ]\n",
      " [4.901683  ]\n",
      " [4.901683  ]\n",
      " [1.9056836 ]\n",
      " [2.6032586 ]\n",
      " [1.9056836 ]\n",
      " [2.2729263 ]\n",
      " [1.8094597 ]\n",
      " [1.0408411 ]\n",
      " [0.76781917]\n",
      " [0.5838635 ]\n",
      " [1.1417242 ]\n",
      " [1.0898733 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [1.6584175 ]\n",
      " [1.6234174 ]\n",
      " [2.4198437 ]\n",
      " [2.05228   ]\n",
      " [1.6217749 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.19448   ]\n",
      " [2.2440648 ]\n",
      " [1.709256  ]\n",
      " [2.116867  ]\n",
      " [1.7611668 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.496809  ]\n",
      " [2.3806388 ]\n",
      " [2.0598502 ]\n",
      " [2.4162536 ]\n",
      " [1.838354  ]\n",
      " [1.9844646 ]\n",
      " [1.4734254 ]\n",
      " [1.4231907 ]\n",
      " [3.2974548 ]\n",
      " [6.5481176 ]\n",
      " [9.986309  ]\n",
      " [3.2493205 ]\n",
      " [3.2493205 ]\n",
      " [3.2493205 ]\n",
      " [3.2493205 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.9555435 ]\n",
      " [1.2314185 ]\n",
      " [1.0777327 ]\n",
      " [1.5221214 ]\n",
      " [1.1774586 ]\n",
      " [1.8292905 ]\n",
      " [1.5858984 ]\n",
      " [2.067986  ]\n",
      " [1.6778183 ]\n",
      " [2.3617883 ]\n",
      " [0.9403962 ]\n",
      " [1.074378  ]\n",
      " [1.2523849 ]\n",
      " [2.049175  ]\n",
      " [1.3940759 ]\n",
      " [2.3168094 ]\n",
      " [0.94018155]\n",
      " [1.9074427 ]\n",
      " [0.6185392 ]\n",
      " [2.9273982 ]\n",
      " [2.8057165 ]\n",
      " [2.1891594 ]\n",
      " [3.687529  ]\n",
      " [2.0759215 ]\n",
      " [1.8814592 ]\n",
      " [2.8036644 ]\n",
      " [1.9702128 ]\n",
      " [2.3573394 ]\n",
      " [1.6848916 ]\n",
      " [2.708887  ]\n",
      " [1.4405506 ]\n",
      " [1.8131044 ]\n",
      " [1.7722043 ]\n",
      " [1.7488335 ]\n",
      " [1.9565536 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [0.8246346 ]\n",
      " [0.7398575 ]\n",
      " [0.47405243]\n",
      " [0.59640545]\n",
      " [1.2614851 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [1.278315  ]\n",
      " [2.3058352 ]\n",
      " [1.4455241 ]\n",
      " [2.2194786 ]\n",
      " [1.9515853 ]\n",
      " [1.2431264 ]\n",
      " [2.3271885 ]\n",
      " [1.5000294 ]\n",
      " [2.0182462 ]\n",
      " [5.2918825 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [1.9755727 ]\n",
      " [1.9462972 ]\n",
      " [1.8128349 ]\n",
      " [3.015459  ]\n",
      " [4.4860563 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [1.2614456 ]\n",
      " [1.180717  ]\n",
      " [1.6261724 ]\n",
      " [1.4672513 ]\n",
      " [1.7269813 ]\n",
      " [4.212178  ]\n",
      " [2.020945  ]\n",
      " [1.8941636 ]\n",
      " [2.8709874 ]\n",
      " [2.976905  ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.7906606 ]\n",
      " [2.693173  ]\n",
      " [2.3420272 ]\n",
      " [2.429106  ]\n",
      " [6.4419336 ]\n",
      " [6.4419336 ]\n",
      " [8.231408  ]\n",
      " [3.274107  ]\n",
      " [3.274107  ]\n",
      " [3.274107  ]\n",
      " [3.274107  ]\n",
      " [3.3444827 ]\n",
      " [3.4067056 ]\n",
      " [3.4912415 ]\n",
      " [5.1840224 ]\n",
      " [5.1840224 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-24 22:44:58,509 [nnabla][INFO]: iter=217 {Training loss}=2.4953930377960205\n",
      "2021-01-24 22:45:16,769 [nnabla][INFO]: iter=219 {Training loss}=2.481599807739258\n",
      "2021-01-24 22:45:35,810 [nnabla][INFO]: iter=221 {Training loss}=2.3745222091674805\n",
      "2021-01-24 22:45:54,583 [nnabla][INFO]: iter=223 {Training loss}=2.2030770778656006\n",
      "2021-01-24 22:46:12,805 [nnabla][INFO]: iter=225 {Training loss}=2.4088687896728516\n",
      "2021-01-24 22:46:31,593 [nnabla][INFO]: iter=227 {Training loss}=2.190786838531494\n",
      "2021-01-24 22:46:49,850 [nnabla][INFO]: iter=229 {Training loss}=2.2634005546569824\n",
      "2021-01-24 22:47:08,539 [nnabla][INFO]: iter=231 {Training loss}=2.2146689891815186\n",
      "2021-01-24 22:47:27,101 [nnabla][INFO]: iter=233 {Training loss}=2.1587462425231934\n",
      "2021-01-24 22:47:45,533 [nnabla][INFO]: iter=235 {Training loss}=2.2149994373321533\n",
      "2021-01-24 22:48:04,417 [nnabla][INFO]: iter=237 {Training loss}=2.1797854900360107\n",
      "2021-01-24 22:48:23,063 [nnabla][INFO]: iter=239 {Training loss}=2.2000820636749268\n",
      "2021-01-24 22:48:41,804 [nnabla][INFO]: iter=241 {Training loss}=2.124096632003784\n",
      "2021-01-24 22:49:00,253 [nnabla][INFO]: iter=243 {Training loss}=2.146798610687256\n",
      "2021-01-24 22:49:18,908 [nnabla][INFO]: iter=245 {Training loss}=2.0693044662475586\n",
      "2021-01-24 22:49:37,505 [nnabla][INFO]: iter=247 {Training loss}=2.152444839477539\n",
      "2021-01-24 22:49:56,041 [nnabla][INFO]: iter=249 {Training loss}=2.2263894081115723\n",
      "2021-01-24 22:50:14,875 [nnabla][INFO]: iter=251 {Training loss}=2.2691941261291504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:6 | Loss:[[0.6885852 ]\n",
      " [0.6599421 ]\n",
      " [0.90025157]\n",
      " [0.64035815]\n",
      " [0.6590691 ]\n",
      " [1.5205681 ]\n",
      " [1.1545854 ]\n",
      " [1.3020995 ]\n",
      " [0.89451015]\n",
      " [1.2847695 ]\n",
      " [2.4653354 ]\n",
      " [2.2874732 ]\n",
      " [2.2368245 ]\n",
      " [3.985735  ]\n",
      " [3.985735  ]\n",
      " [1.6233348 ]\n",
      " [1.8216376 ]\n",
      " [1.6233348 ]\n",
      " [1.7315049 ]\n",
      " [1.5504115 ]\n",
      " [0.9667575 ]\n",
      " [0.60521305]\n",
      " [0.58825755]\n",
      " [0.9594463 ]\n",
      " [0.90549725]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [1.2814276 ]\n",
      " [1.1187828 ]\n",
      " [1.4825041 ]\n",
      " [2.01951   ]\n",
      " [1.3547215 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [1.6174595 ]\n",
      " [1.7850347 ]\n",
      " [1.7504197 ]\n",
      " [1.7725718 ]\n",
      " [2.5137522 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.2633326 ]\n",
      " [2.3170729 ]\n",
      " [1.5210855 ]\n",
      " [2.2493625 ]\n",
      " [1.5101919 ]\n",
      " [2.227714  ]\n",
      " [2.3130138 ]\n",
      " [2.1011307 ]\n",
      " [2.4105163 ]\n",
      " [4.288786  ]\n",
      " [8.947333  ]\n",
      " [3.3045347 ]\n",
      " [3.3045347 ]\n",
      " [3.3045347 ]\n",
      " [3.3045347 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [1.388793  ]\n",
      " [1.2461225 ]\n",
      " [1.2165016 ]\n",
      " [0.7642271 ]\n",
      " [0.9439902 ]\n",
      " [2.1885493 ]\n",
      " [1.0069299 ]\n",
      " [2.7101936 ]\n",
      " [1.4112793 ]\n",
      " [1.2892632 ]\n",
      " [0.97635907]\n",
      " [1.1556494 ]\n",
      " [1.2277336 ]\n",
      " [0.7728412 ]\n",
      " [0.8355712 ]\n",
      " [1.0571166 ]\n",
      " [1.4763912 ]\n",
      " [1.623831  ]\n",
      " [1.6253393 ]\n",
      " [2.0416126 ]\n",
      " [1.4146178 ]\n",
      " [1.5003941 ]\n",
      " [1.4680032 ]\n",
      " [1.640266  ]\n",
      " [1.5251513 ]\n",
      " [1.3835139 ]\n",
      " [1.567498  ]\n",
      " [1.4611139 ]\n",
      " [1.4775668 ]\n",
      " [1.3879182 ]\n",
      " [1.5549648 ]\n",
      " [2.0889785 ]\n",
      " [3.3188386 ]\n",
      " [1.5280329 ]\n",
      " [1.1390066 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [0.15402445]\n",
      " [0.21519704]\n",
      " [0.16288003]\n",
      " [1.0816336 ]\n",
      " [0.34294948]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [0.63783073]\n",
      " [0.84428215]\n",
      " [2.3033354 ]\n",
      " [0.61889994]\n",
      " [0.9619637 ]\n",
      " [1.5526578 ]\n",
      " [1.9989568 ]\n",
      " [1.759825  ]\n",
      " [1.8100965 ]\n",
      " [1.8425255 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [1.7102914 ]\n",
      " [1.8066095 ]\n",
      " [1.6030157 ]\n",
      " [1.7474933 ]\n",
      " [2.3964517 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [1.0301595 ]\n",
      " [1.0962696 ]\n",
      " [3.183052  ]\n",
      " [2.397025  ]\n",
      " [1.3373547 ]\n",
      " [2.0349855 ]\n",
      " [1.7385803 ]\n",
      " [1.9406829 ]\n",
      " [2.6338365 ]\n",
      " [1.8486016 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.8329802 ]\n",
      " [2.4798956 ]\n",
      " [1.495362  ]\n",
      " [1.5060943 ]\n",
      " [7.232148  ]\n",
      " [7.232148  ]\n",
      " [8.986865  ]\n",
      " [3.3122423 ]\n",
      " [3.3122423 ]\n",
      " [3.3122423 ]\n",
      " [3.3122423 ]\n",
      " [2.4859958 ]\n",
      " [3.5233102 ]\n",
      " [2.2450476 ]\n",
      " [4.891822  ]\n",
      " [4.891822  ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-24 22:50:33,418 [nnabla][INFO]: iter=253 {Training loss}=2.080251932144165\n",
      "2021-01-24 22:50:52,024 [nnabla][INFO]: iter=255 {Training loss}=1.9834179878234863\n",
      "2021-01-24 22:51:10,841 [nnabla][INFO]: iter=257 {Training loss}=2.1022462844848633\n",
      "2021-01-24 22:51:29,568 [nnabla][INFO]: iter=259 {Training loss}=2.0915255546569824\n",
      "2021-01-24 22:51:47,986 [nnabla][INFO]: iter=261 {Training loss}=2.1510138511657715\n",
      "2021-01-24 22:52:06,272 [nnabla][INFO]: iter=263 {Training loss}=1.9472815990447998\n",
      "2021-01-24 22:52:24,639 [nnabla][INFO]: iter=265 {Training loss}=2.1909265518188477\n",
      "2021-01-24 22:52:43,312 [nnabla][INFO]: iter=267 {Training loss}=2.006927728652954\n",
      "2021-01-24 22:53:01,871 [nnabla][INFO]: iter=269 {Training loss}=1.8223159313201904\n",
      "2021-01-24 22:53:20,357 [nnabla][INFO]: iter=271 {Training loss}=2.031790256500244\n",
      "2021-01-24 22:53:38,435 [nnabla][INFO]: iter=273 {Training loss}=2.0495519638061523\n",
      "2021-01-24 22:53:56,753 [nnabla][INFO]: iter=275 {Training loss}=2.069704055786133\n",
      "2021-01-24 22:54:15,285 [nnabla][INFO]: iter=277 {Training loss}=1.966855525970459\n",
      "2021-01-24 22:54:34,095 [nnabla][INFO]: iter=279 {Training loss}=2.0873911380767822\n",
      "2021-01-24 22:54:52,411 [nnabla][INFO]: iter=281 {Training loss}=2.1442878246307373\n",
      "2021-01-24 22:55:10,474 [nnabla][INFO]: iter=283 {Training loss}=2.165565252304077\n",
      "2021-01-24 22:55:28,881 [nnabla][INFO]: iter=285 {Training loss}=2.0887508392333984\n",
      "2021-01-24 22:55:47,378 [nnabla][INFO]: iter=287 {Training loss}=2.3342175483703613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:7 | Loss:[[0.8404051 ]\n",
      " [0.8045059 ]\n",
      " [0.94287086]\n",
      " [0.8952632 ]\n",
      " [0.81243855]\n",
      " [1.703419  ]\n",
      " [1.0702078 ]\n",
      " [2.0673652 ]\n",
      " [1.0085115 ]\n",
      " [1.0857433 ]\n",
      " [3.1254947 ]\n",
      " [1.9278864 ]\n",
      " [2.1059341 ]\n",
      " [6.249978  ]\n",
      " [6.249978  ]\n",
      " [2.028489  ]\n",
      " [2.5169873 ]\n",
      " [2.028489  ]\n",
      " [1.6994882 ]\n",
      " [1.2636812 ]\n",
      " [1.8192586 ]\n",
      " [0.8830267 ]\n",
      " [0.6565179 ]\n",
      " [1.1116371 ]\n",
      " [0.89621836]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [1.0286398 ]\n",
      " [0.9573944 ]\n",
      " [1.6613913 ]\n",
      " [1.677447  ]\n",
      " [2.330995  ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.3630238 ]\n",
      " [1.668496  ]\n",
      " [1.6397144 ]\n",
      " [1.8858352 ]\n",
      " [2.1449728 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [1.7839005 ]\n",
      " [2.1539025 ]\n",
      " [1.4689889 ]\n",
      " [2.0547657 ]\n",
      " [1.1840345 ]\n",
      " [1.0633032 ]\n",
      " [1.7202182 ]\n",
      " [1.0072916 ]\n",
      " [1.0988793 ]\n",
      " [6.6780376 ]\n",
      " [8.911815  ]\n",
      " [3.217642  ]\n",
      " [3.217642  ]\n",
      " [3.217642  ]\n",
      " [3.217642  ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [6.316147  ]\n",
      " [0.32633746]\n",
      " [0.64767843]\n",
      " [0.88384086]\n",
      " [0.3213623 ]\n",
      " [2.271719  ]\n",
      " [0.94829595]\n",
      " [2.0905437 ]\n",
      " [1.0567745 ]\n",
      " [3.871119  ]\n",
      " [0.8366742 ]\n",
      " [0.79361385]\n",
      " [1.9462165 ]\n",
      " [1.0225314 ]\n",
      " [1.0028853 ]\n",
      " [0.8947226 ]\n",
      " [1.5516932 ]\n",
      " [1.258693  ]\n",
      " [1.3971201 ]\n",
      " [1.165313  ]\n",
      " [2.535383  ]\n",
      " [1.364425  ]\n",
      " [3.5900397 ]\n",
      " [1.2382017 ]\n",
      " [1.0870275 ]\n",
      " [4.332369  ]\n",
      " [1.0402702 ]\n",
      " [0.9115558 ]\n",
      " [0.8826607 ]\n",
      " [1.6846136 ]\n",
      " [0.99401224]\n",
      " [2.3571253 ]\n",
      " [1.935555  ]\n",
      " [1.4608423 ]\n",
      " [0.8914354 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [0.6193783 ]\n",
      " [0.48144963]\n",
      " [0.43016386]\n",
      " [0.42640594]\n",
      " [0.44615075]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [1.0282738 ]\n",
      " [1.3744335 ]\n",
      " [1.6681676 ]\n",
      " [1.022965  ]\n",
      " [1.9025561 ]\n",
      " [1.3912982 ]\n",
      " [2.3867874 ]\n",
      " [0.6864606 ]\n",
      " [1.0326248 ]\n",
      " [1.1071213 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [0.9390192 ]\n",
      " [1.4008532 ]\n",
      " [0.9216654 ]\n",
      " [1.4092884 ]\n",
      " [1.3747616 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [1.0959048 ]\n",
      " [1.0450686 ]\n",
      " [3.0299573 ]\n",
      " [2.3436997 ]\n",
      " [1.3958173 ]\n",
      " [1.2487344 ]\n",
      " [5.473885  ]\n",
      " [3.0817752 ]\n",
      " [5.8905296 ]\n",
      " [2.4754872 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [2.7707562 ]\n",
      " [1.8477105 ]\n",
      " [2.2741241 ]\n",
      " [1.813014  ]\n",
      " [6.3281784 ]\n",
      " [6.3281784 ]\n",
      " [8.112829  ]\n",
      " [3.2398024 ]\n",
      " [3.2398024 ]\n",
      " [3.2398024 ]\n",
      " [3.2398024 ]\n",
      " [4.285091  ]\n",
      " [1.9679015 ]\n",
      " [2.7478673 ]\n",
      " [6.519787  ]\n",
      " [6.519787  ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-24 22:56:06,069 [nnabla][INFO]: iter=289 {Training loss}=2.02249813079834\n",
      "2021-01-24 22:56:24,482 [nnabla][INFO]: iter=291 {Training loss}=1.9323519468307495\n",
      "2021-01-24 22:56:42,351 [nnabla][INFO]: iter=293 {Training loss}=1.9301341772079468\n",
      "2021-01-24 22:57:01,198 [nnabla][INFO]: iter=295 {Training loss}=1.9536226987838745\n",
      "2021-01-24 22:57:19,964 [nnabla][INFO]: iter=297 {Training loss}=2.13161563873291\n",
      "2021-01-24 22:57:38,476 [nnabla][INFO]: iter=299 {Training loss}=1.8499536514282227\n",
      "2021-01-24 22:57:56,596 [nnabla][INFO]: iter=301 {Training loss}=1.9764893054962158\n",
      "2021-01-24 22:58:15,312 [nnabla][INFO]: iter=303 {Training loss}=1.9501385688781738\n",
      "2021-01-24 22:58:33,907 [nnabla][INFO]: iter=305 {Training loss}=1.650172472000122\n",
      "2021-01-24 22:58:52,108 [nnabla][INFO]: iter=307 {Training loss}=1.964810848236084\n",
      "2021-01-24 22:59:10,458 [nnabla][INFO]: iter=309 {Training loss}=2.0918703079223633\n",
      "2021-01-24 22:59:28,931 [nnabla][INFO]: iter=311 {Training loss}=2.076798915863037\n",
      "2021-01-24 22:59:47,494 [nnabla][INFO]: iter=313 {Training loss}=1.8904434442520142\n",
      "2021-01-24 23:00:05,946 [nnabla][INFO]: iter=315 {Training loss}=1.990211844444275\n",
      "2021-01-24 23:00:24,359 [nnabla][INFO]: iter=317 {Training loss}=2.014796733856201\n",
      "2021-01-24 23:00:42,646 [nnabla][INFO]: iter=319 {Training loss}=1.959956407546997\n",
      "2021-01-24 23:01:00,828 [nnabla][INFO]: iter=321 {Training loss}=2.110239267349243\n",
      "2021-01-24 23:01:18,777 [nnabla][INFO]: iter=323 {Training loss}=2.1828606128692627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:8 | Loss:[[1.4976851 ]\n",
      " [0.5038369 ]\n",
      " [0.55566454]\n",
      " [0.78086275]\n",
      " [0.5696901 ]\n",
      " [2.1124234 ]\n",
      " [0.9893247 ]\n",
      " [0.7374017 ]\n",
      " [0.8857727 ]\n",
      " [1.0580837 ]\n",
      " [3.81871   ]\n",
      " [1.941451  ]\n",
      " [2.133133  ]\n",
      " [4.555847  ]\n",
      " [4.555847  ]\n",
      " [1.644582  ]\n",
      " [1.4486098 ]\n",
      " [1.644582  ]\n",
      " [1.799379  ]\n",
      " [1.605095  ]\n",
      " [0.79074734]\n",
      " [0.95864636]\n",
      " [0.9342938 ]\n",
      " [1.0528402 ]\n",
      " [1.017729  ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [0.96504194]\n",
      " [0.88320416]\n",
      " [1.71729   ]\n",
      " [1.8769866 ]\n",
      " [1.050518  ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [1.5867705 ]\n",
      " [1.5788395 ]\n",
      " [1.7355663 ]\n",
      " [2.1347206 ]\n",
      " [1.6589916 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [1.1341747 ]\n",
      " [1.3751597 ]\n",
      " [1.3520653 ]\n",
      " [1.7085028 ]\n",
      " [1.2601383 ]\n",
      " [1.5761534 ]\n",
      " [1.4528714 ]\n",
      " [1.4455427 ]\n",
      " [1.5476091 ]\n",
      " [5.03172   ]\n",
      " [8.992189  ]\n",
      " [3.2637913 ]\n",
      " [3.2637913 ]\n",
      " [3.2637913 ]\n",
      " [3.2637913 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [1.6993157 ]\n",
      " [0.84370726]\n",
      " [1.340538  ]\n",
      " [1.551634  ]\n",
      " [0.64295065]\n",
      " [2.0483305 ]\n",
      " [0.98521507]\n",
      " [3.1475844 ]\n",
      " [1.2718581 ]\n",
      " [1.4571201 ]\n",
      " [0.8308951 ]\n",
      " [0.9424502 ]\n",
      " [3.8589783 ]\n",
      " [1.8353546 ]\n",
      " [1.2829869 ]\n",
      " [0.75060135]\n",
      " [2.438223  ]\n",
      " [0.6514582 ]\n",
      " [0.60749996]\n",
      " [1.3222349 ]\n",
      " [1.9178163 ]\n",
      " [1.1031355 ]\n",
      " [2.3778677 ]\n",
      " [0.9923547 ]\n",
      " [1.2148937 ]\n",
      " [0.562431  ]\n",
      " [0.9538515 ]\n",
      " [0.38041052]\n",
      " [0.751802  ]\n",
      " [1.1599135 ]\n",
      " [1.2146074 ]\n",
      " [1.3515979 ]\n",
      " [2.7921107 ]\n",
      " [1.2760606 ]\n",
      " [1.4167372 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [0.53590566]\n",
      " [0.3854465 ]\n",
      " [0.3197814 ]\n",
      " [0.29196927]\n",
      " [0.294847  ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [0.16669518]\n",
      " [0.17006962]\n",
      " [0.75631714]\n",
      " [0.11063156]\n",
      " [0.30992594]\n",
      " [0.9480126 ]\n",
      " [0.422572  ]\n",
      " [0.44425625]\n",
      " [0.4427714 ]\n",
      " [2.8419771 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [0.80760515]\n",
      " [2.1802633 ]\n",
      " [0.8233109 ]\n",
      " [3.212994  ]\n",
      " [1.2005668 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [1.1380111 ]\n",
      " [0.83094126]\n",
      " [2.2059138 ]\n",
      " [1.4469411 ]\n",
      " [1.2342346 ]\n",
      " [1.7722131 ]\n",
      " [0.9104561 ]\n",
      " [1.6371808 ]\n",
      " [4.637084  ]\n",
      " [3.6405082 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [2.7948782 ]\n",
      " [3.5101824 ]\n",
      " [1.3571813 ]\n",
      " [1.4761987 ]\n",
      " [6.7963047 ]\n",
      " [6.7963047 ]\n",
      " [6.6187396 ]\n",
      " [3.2602324 ]\n",
      " [3.2602324 ]\n",
      " [3.2602324 ]\n",
      " [3.2602324 ]\n",
      " [4.274879  ]\n",
      " [3.7280056 ]\n",
      " [2.1202655 ]\n",
      " [5.70313   ]\n",
      " [5.70313   ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-24 23:01:37,514 [nnabla][INFO]: iter=325 {Training loss}=1.989101529121399\n",
      "2021-01-24 23:01:56,138 [nnabla][INFO]: iter=327 {Training loss}=1.9468700885772705\n",
      "2021-01-24 23:02:14,725 [nnabla][INFO]: iter=329 {Training loss}=1.9108717441558838\n",
      "2021-01-24 23:02:32,517 [nnabla][INFO]: iter=331 {Training loss}=1.8216438293457031\n",
      "2021-01-24 23:02:50,570 [nnabla][INFO]: iter=333 {Training loss}=1.9280434846878052\n",
      "2021-01-24 23:03:08,701 [nnabla][INFO]: iter=335 {Training loss}=1.779225468635559\n",
      "2021-01-24 23:03:27,359 [nnabla][INFO]: iter=337 {Training loss}=1.9529849290847778\n",
      "2021-01-24 23:03:46,256 [nnabla][INFO]: iter=339 {Training loss}=2.062739372253418\n",
      "2021-01-24 23:04:04,691 [nnabla][INFO]: iter=341 {Training loss}=1.680403709411621\n",
      "2021-01-24 23:04:22,741 [nnabla][INFO]: iter=343 {Training loss}=1.785764455795288\n",
      "2021-01-24 23:04:41,075 [nnabla][INFO]: iter=345 {Training loss}=1.9452835321426392\n",
      "2021-01-24 23:04:59,181 [nnabla][INFO]: iter=347 {Training loss}=1.892675518989563\n",
      "2021-01-24 23:05:17,640 [nnabla][INFO]: iter=349 {Training loss}=1.8267461061477661\n",
      "2021-01-24 23:05:35,464 [nnabla][INFO]: iter=351 {Training loss}=1.9744206666946411\n",
      "2021-01-24 23:05:53,677 [nnabla][INFO]: iter=353 {Training loss}=1.8831361532211304\n",
      "2021-01-24 23:06:12,504 [nnabla][INFO]: iter=355 {Training loss}=1.9808520078659058\n",
      "2021-01-24 23:06:30,563 [nnabla][INFO]: iter=357 {Training loss}=2.0152480602264404\n",
      "2021-01-24 23:06:49,033 [nnabla][INFO]: iter=359 {Training loss}=2.1670150756835938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:9 | Loss:[[0.735772  ]\n",
      " [0.7568685 ]\n",
      " [0.7644828 ]\n",
      " [1.0019166 ]\n",
      " [0.68540907]\n",
      " [2.224726  ]\n",
      " [0.89630705]\n",
      " [0.87936944]\n",
      " [1.0722183 ]\n",
      " [1.0634328 ]\n",
      " [3.736922  ]\n",
      " [1.6910628 ]\n",
      " [1.704981  ]\n",
      " [5.1090965 ]\n",
      " [5.1090965 ]\n",
      " [1.982861  ]\n",
      " [1.7300258 ]\n",
      " [1.982861  ]\n",
      " [1.7978369 ]\n",
      " [1.3525717 ]\n",
      " [0.48356146]\n",
      " [0.66606283]\n",
      " [0.72066444]\n",
      " [1.29175   ]\n",
      " [1.1288953 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [0.7566956 ]\n",
      " [0.6696903 ]\n",
      " [2.754695  ]\n",
      " [1.4207226 ]\n",
      " [1.4112197 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [1.176129  ]\n",
      " [1.1193297 ]\n",
      " [1.5628341 ]\n",
      " [1.5803392 ]\n",
      " [2.589725  ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [1.4336237 ]\n",
      " [2.0939655 ]\n",
      " [1.153488  ]\n",
      " [1.8765235 ]\n",
      " [1.1869404 ]\n",
      " [1.435534  ]\n",
      " [1.7303599 ]\n",
      " [1.4093484 ]\n",
      " [2.414369  ]\n",
      " [5.1929884 ]\n",
      " [9.470972  ]\n",
      " [3.2516088 ]\n",
      " [3.2516088 ]\n",
      " [3.2516088 ]\n",
      " [3.2516088 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [5.752385  ]\n",
      " [0.45466092]\n",
      " [1.0854998 ]\n",
      " [0.64438146]\n",
      " [0.4599374 ]\n",
      " [3.2419496 ]\n",
      " [1.1067036 ]\n",
      " [1.2592171 ]\n",
      " [0.77904195]\n",
      " [3.0053947 ]\n",
      " [0.6779036 ]\n",
      " [0.6967026 ]\n",
      " [0.7495968 ]\n",
      " [1.1043823 ]\n",
      " [0.7206758 ]\n",
      " [0.3484874 ]\n",
      " [0.72612935]\n",
      " [1.6976962 ]\n",
      " [0.40610498]\n",
      " [1.7682861 ]\n",
      " [1.3332332 ]\n",
      " [0.8089258 ]\n",
      " [1.0991173 ]\n",
      " [0.6542975 ]\n",
      " [0.7048066 ]\n",
      " [1.7767534 ]\n",
      " [0.5451856 ]\n",
      " [0.582146  ]\n",
      " [0.5990559 ]\n",
      " [1.3658661 ]\n",
      " [0.48224252]\n",
      " [2.3280694 ]\n",
      " [0.7975998 ]\n",
      " [3.096162  ]\n",
      " [1.0278325 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [0.04047835]\n",
      " [0.03371908]\n",
      " [0.02565673]\n",
      " [0.03467583]\n",
      " [0.03758204]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [0.6222853 ]\n",
      " [0.77065176]\n",
      " [1.3561978 ]\n",
      " [1.5586882 ]\n",
      " [1.4417274 ]\n",
      " [0.1927912 ]\n",
      " [0.2372231 ]\n",
      " [0.5076016 ]\n",
      " [0.20614542]\n",
      " [0.70947737]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [0.54483217]\n",
      " [1.0076871 ]\n",
      " [0.68674994]\n",
      " [2.4777393 ]\n",
      " [1.5685174 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [0.96967965]\n",
      " [1.1157353 ]\n",
      " [2.1037526 ]\n",
      " [0.9072493 ]\n",
      " [1.0447063 ]\n",
      " [1.1260853 ]\n",
      " [1.2570381 ]\n",
      " [2.3490672 ]\n",
      " [4.7093577 ]\n",
      " [1.8427207 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.7765398 ]\n",
      " [2.326807  ]\n",
      " [3.0771413 ]\n",
      " [2.3757489 ]\n",
      " [8.478494  ]\n",
      " [8.478494  ]\n",
      " [8.152515  ]\n",
      " [3.265455  ]\n",
      " [3.265455  ]\n",
      " [3.265455  ]\n",
      " [3.265455  ]\n",
      " [1.9406319 ]\n",
      " [2.768974  ]\n",
      " [1.4658533 ]\n",
      " [6.765908  ]\n",
      " [6.765908  ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-24 23:07:06,883 [nnabla][INFO]: iter=361 {Training loss}=1.8513637781143188\n",
      "2021-01-24 23:07:25,123 [nnabla][INFO]: iter=363 {Training loss}=1.9512077569961548\n",
      "2021-01-24 23:07:43,714 [nnabla][INFO]: iter=365 {Training loss}=2.003016710281372\n",
      "2021-01-24 23:08:02,438 [nnabla][INFO]: iter=367 {Training loss}=1.835451602935791\n",
      "2021-01-24 23:08:20,625 [nnabla][INFO]: iter=369 {Training loss}=2.0967798233032227\n",
      "2021-01-24 23:08:38,395 [nnabla][INFO]: iter=371 {Training loss}=1.79111647605896\n",
      "2021-01-24 23:08:56,384 [nnabla][INFO]: iter=373 {Training loss}=1.8644229173660278\n",
      "2021-01-24 23:09:14,821 [nnabla][INFO]: iter=375 {Training loss}=1.794245958328247\n",
      "2021-01-24 23:09:32,811 [nnabla][INFO]: iter=377 {Training loss}=1.6980174779891968\n",
      "2021-01-24 23:09:51,247 [nnabla][INFO]: iter=379 {Training loss}=1.8785220384597778\n",
      "2021-01-24 23:10:09,066 [nnabla][INFO]: iter=381 {Training loss}=1.9091925621032715\n",
      "2021-01-24 23:10:26,802 [nnabla][INFO]: iter=383 {Training loss}=1.855940580368042\n",
      "2021-01-24 23:10:45,534 [nnabla][INFO]: iter=385 {Training loss}=1.6631489992141724\n",
      "2021-01-24 23:11:03,700 [nnabla][INFO]: iter=387 {Training loss}=1.7922048568725586\n",
      "2021-01-24 23:11:22,009 [nnabla][INFO]: iter=389 {Training loss}=1.8595058917999268\n",
      "2021-01-24 23:11:40,266 [nnabla][INFO]: iter=391 {Training loss}=2.017793655395508\n",
      "2021-01-24 23:11:58,254 [nnabla][INFO]: iter=393 {Training loss}=1.9386943578720093\n",
      "2021-01-24 23:12:16,801 [nnabla][INFO]: iter=395 {Training loss}=2.205113410949707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:10 | Loss:[[0.8181421 ]\n",
      " [0.70629746]\n",
      " [0.78228676]\n",
      " [0.85317993]\n",
      " [0.64058584]\n",
      " [1.5894765 ]\n",
      " [1.3379486 ]\n",
      " [1.0213445 ]\n",
      " [1.4856588 ]\n",
      " [1.2049847 ]\n",
      " [4.194795  ]\n",
      " [1.6789435 ]\n",
      " [1.537419  ]\n",
      " [5.7998114 ]\n",
      " [5.7998114 ]\n",
      " [2.1153846 ]\n",
      " [1.9948545 ]\n",
      " [2.1153846 ]\n",
      " [1.5085258 ]\n",
      " [1.4641023 ]\n",
      " [0.8301855 ]\n",
      " [0.80217683]\n",
      " [0.77631545]\n",
      " [1.029909  ]\n",
      " [0.94426537]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [1.2144964 ]\n",
      " [1.0225186 ]\n",
      " [1.380947  ]\n",
      " [1.7135175 ]\n",
      " [1.0656087 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [1.4768844 ]\n",
      " [1.5190903 ]\n",
      " [1.9295015 ]\n",
      " [1.8740503 ]\n",
      " [1.3813621 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [1.5115511 ]\n",
      " [2.337952  ]\n",
      " [0.82214105]\n",
      " [2.1587934 ]\n",
      " [1.8415924 ]\n",
      " [1.4299    ]\n",
      " [1.601745  ]\n",
      " [1.370414  ]\n",
      " [2.1802595 ]\n",
      " [4.9541373 ]\n",
      " [8.249123  ]\n",
      " [3.2065327 ]\n",
      " [3.2065327 ]\n",
      " [3.2065327 ]\n",
      " [3.2065327 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [1.9071813 ]\n",
      " [0.15176262]\n",
      " [0.98013467]\n",
      " [0.10774114]\n",
      " [0.09419399]\n",
      " [3.0509725 ]\n",
      " [1.0294569 ]\n",
      " [1.3538532 ]\n",
      " [0.7510856 ]\n",
      " [2.1735647 ]\n",
      " [0.86600226]\n",
      " [0.70458406]\n",
      " [1.4316834 ]\n",
      " [0.8083133 ]\n",
      " [1.0111246 ]\n",
      " [0.88167274]\n",
      " [1.059673  ]\n",
      " [1.2145616 ]\n",
      " [0.90839785]\n",
      " [1.1483133 ]\n",
      " [1.2044322 ]\n",
      " [0.94501144]\n",
      " [2.6920588 ]\n",
      " [0.9335903 ]\n",
      " [1.003367  ]\n",
      " [2.3425436 ]\n",
      " [1.4574988 ]\n",
      " [0.65188444]\n",
      " [0.5353979 ]\n",
      " [0.7714432 ]\n",
      " [0.59278595]\n",
      " [1.6586194 ]\n",
      " [2.1513386 ]\n",
      " [1.5431479 ]\n",
      " [1.0271914 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [0.11583012]\n",
      " [0.24014461]\n",
      " [0.09310978]\n",
      " [0.32810208]\n",
      " [0.12298744]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [0.74843067]\n",
      " [0.30785358]\n",
      " [2.04148   ]\n",
      " [0.8319084 ]\n",
      " [0.6433799 ]\n",
      " [3.3382077 ]\n",
      " [0.5827814 ]\n",
      " [0.7179437 ]\n",
      " [3.3662982 ]\n",
      " [3.761468  ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [1.3878155 ]\n",
      " [1.302171  ]\n",
      " [1.4675274 ]\n",
      " [2.6720207 ]\n",
      " [1.6259536 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [0.770399  ]\n",
      " [0.7672241 ]\n",
      " [2.0182905 ]\n",
      " [1.1023631 ]\n",
      " [0.80244124]\n",
      " [2.690236  ]\n",
      " [2.3544948 ]\n",
      " [2.16364   ]\n",
      " [3.8864303 ]\n",
      " [2.5414333 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [2.7786791 ]\n",
      " [1.9606907 ]\n",
      " [1.5514375 ]\n",
      " [1.2229435 ]\n",
      " [7.328745  ]\n",
      " [7.328745  ]\n",
      " [7.81938   ]\n",
      " [3.2149396 ]\n",
      " [3.2149396 ]\n",
      " [3.2149396 ]\n",
      " [3.2149396 ]\n",
      " [3.2929523 ]\n",
      " [2.1041317 ]\n",
      " [2.9245574 ]\n",
      " [6.645667  ]\n",
      " [6.645667  ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-24 23:12:35,618 [nnabla][INFO]: iter=397 {Training loss}=1.800217628479004\n",
      "2021-01-24 23:12:53,960 [nnabla][INFO]: iter=399 {Training loss}=1.7917895317077637\n",
      "2021-01-24 23:13:11,744 [nnabla][INFO]: iter=401 {Training loss}=1.7820144891738892\n",
      "2021-01-24 23:13:29,988 [nnabla][INFO]: iter=403 {Training loss}=1.702552318572998\n",
      "2021-01-24 23:13:48,188 [nnabla][INFO]: iter=405 {Training loss}=1.8415051698684692\n",
      "2021-01-24 23:14:06,289 [nnabla][INFO]: iter=407 {Training loss}=1.6888320446014404\n",
      "2021-01-24 23:14:24,311 [nnabla][INFO]: iter=409 {Training loss}=1.7250511646270752\n",
      "2021-01-24 23:14:42,162 [nnabla][INFO]: iter=411 {Training loss}=1.8688864707946777\n",
      "2021-01-24 23:15:00,327 [nnabla][INFO]: iter=413 {Training loss}=1.576442003250122\n",
      "2021-01-24 23:15:19,005 [nnabla][INFO]: iter=415 {Training loss}=1.7714760303497314\n",
      "2021-01-24 23:15:37,354 [nnabla][INFO]: iter=417 {Training loss}=1.8590307235717773\n",
      "2021-01-24 23:15:55,655 [nnabla][INFO]: iter=419 {Training loss}=1.8665175437927246\n",
      "2021-01-24 23:16:14,447 [nnabla][INFO]: iter=421 {Training loss}=1.6413235664367676\n",
      "2021-01-24 23:16:33,389 [nnabla][INFO]: iter=423 {Training loss}=1.7020334005355835\n",
      "2021-01-24 23:16:52,187 [nnabla][INFO]: iter=425 {Training loss}=1.67169988155365\n",
      "2021-01-24 23:17:10,867 [nnabla][INFO]: iter=427 {Training loss}=1.8415600061416626\n",
      "2021-01-24 23:17:29,797 [nnabla][INFO]: iter=429 {Training loss}=1.803473711013794\n",
      "2021-01-24 23:17:47,928 [nnabla][INFO]: iter=431 {Training loss}=2.103410243988037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:11 | Loss:[[0.3953639 ]\n",
      " [0.33843428]\n",
      " [0.91672915]\n",
      " [1.0473387 ]\n",
      " [0.44820696]\n",
      " [1.8198729 ]\n",
      " [1.310638  ]\n",
      " [1.2389731 ]\n",
      " [1.5414246 ]\n",
      " [1.8284538 ]\n",
      " [2.885965  ]\n",
      " [1.8687253 ]\n",
      " [2.0324838 ]\n",
      " [6.8370433 ]\n",
      " [6.8370433 ]\n",
      " [1.1625589 ]\n",
      " [1.7739502 ]\n",
      " [1.1625589 ]\n",
      " [1.4090102 ]\n",
      " [1.0495129 ]\n",
      " [1.0829002 ]\n",
      " [1.0635049 ]\n",
      " [0.8602967 ]\n",
      " [1.1522155 ]\n",
      " [1.9838278 ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [0.43301198]\n",
      " [0.33543125]\n",
      " [1.00128   ]\n",
      " [0.9726446 ]\n",
      " [0.69916403]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [1.1829156 ]\n",
      " [1.4327763 ]\n",
      " [1.2352377 ]\n",
      " [3.1436071 ]\n",
      " [0.8303665 ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [1.194387  ]\n",
      " [1.3678249 ]\n",
      " [0.83168983]\n",
      " [2.8493054 ]\n",
      " [0.85905915]\n",
      " [0.8588538 ]\n",
      " [1.0601313 ]\n",
      " [0.62091064]\n",
      " [0.9463294 ]\n",
      " [7.711976  ]\n",
      " [9.272906  ]\n",
      " [3.2376723 ]\n",
      " [3.2376723 ]\n",
      " [3.2376723 ]\n",
      " [3.2376723 ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [1.422729  ]\n",
      " [0.33469597]\n",
      " [0.4153893 ]\n",
      " [0.24393824]\n",
      " [0.5617838 ]\n",
      " [4.0432606 ]\n",
      " [0.75322324]\n",
      " [1.2510037 ]\n",
      " [0.72080964]\n",
      " [2.670825  ]\n",
      " [0.49028823]\n",
      " [0.54482704]\n",
      " [0.59304005]\n",
      " [0.5875109 ]\n",
      " [1.4414065 ]\n",
      " [0.8895636 ]\n",
      " [1.3761616 ]\n",
      " [1.7852733 ]\n",
      " [1.065885  ]\n",
      " [1.5132667 ]\n",
      " [1.3047884 ]\n",
      " [1.5405643 ]\n",
      " [2.8481154 ]\n",
      " [0.72730654]\n",
      " [0.7842831 ]\n",
      " [0.30780196]\n",
      " [0.40180275]\n",
      " [0.24034974]\n",
      " [0.31241682]\n",
      " [0.3293474 ]\n",
      " [0.64151144]\n",
      " [0.6182464 ]\n",
      " [1.3174582 ]\n",
      " [0.7901178 ]\n",
      " [0.58456576]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [0.14728294]\n",
      " [1.0241315 ]\n",
      " [0.01661945]\n",
      " [0.02083721]\n",
      " [0.01686669]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [0.6783104 ]\n",
      " [0.4600399 ]\n",
      " [1.152362  ]\n",
      " [0.29660913]\n",
      " [0.8257291 ]\n",
      " [0.21699792]\n",
      " [0.16605403]\n",
      " [0.14047465]\n",
      " [0.40407664]\n",
      " [0.1174069 ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [1.3111415 ]\n",
      " [0.63000757]\n",
      " [1.3803439 ]\n",
      " [0.9957819 ]\n",
      " [0.8378969 ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [0.8515388 ]\n",
      " [0.8028546 ]\n",
      " [3.4041374 ]\n",
      " [1.0204405 ]\n",
      " [1.2551191 ]\n",
      " [4.0882998 ]\n",
      " [1.5930761 ]\n",
      " [1.6215497 ]\n",
      " [4.4899063 ]\n",
      " [1.6903535 ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [2.757369  ]\n",
      " [1.4855778 ]\n",
      " [2.045224  ]\n",
      " [1.461712  ]\n",
      " [7.1316686 ]\n",
      " [7.1316686 ]\n",
      " [8.311785  ]\n",
      " [3.2441843 ]\n",
      " [3.2441843 ]\n",
      " [3.2441843 ]\n",
      " [3.2441843 ]\n",
      " [3.2426538 ]\n",
      " [2.8660483 ]\n",
      " [2.9197164 ]\n",
      " [7.5730553 ]\n",
      " [7.5730553 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-24 23:18:06,899 [nnabla][INFO]: iter=433 {Training loss}=1.7299307584762573\n",
      "2021-01-24 23:18:25,544 [nnabla][INFO]: iter=435 {Training loss}=1.6940972805023193\n",
      "2021-01-24 23:18:44,322 [nnabla][INFO]: iter=437 {Training loss}=1.7770376205444336\n",
      "2021-01-24 23:19:02,889 [nnabla][INFO]: iter=439 {Training loss}=1.5955986976623535\n",
      "2021-01-24 23:19:21,443 [nnabla][INFO]: iter=441 {Training loss}=1.7869294881820679\n",
      "2021-01-24 23:19:40,075 [nnabla][INFO]: iter=443 {Training loss}=1.5917391777038574\n",
      "2021-01-24 23:19:58,990 [nnabla][INFO]: iter=445 {Training loss}=1.8222922086715698\n",
      "2021-01-24 23:20:17,551 [nnabla][INFO]: iter=447 {Training loss}=1.955286979675293\n",
      "2021-01-24 23:20:36,393 [nnabla][INFO]: iter=449 {Training loss}=1.7391265630722046\n",
      "2021-01-24 23:20:55,447 [nnabla][INFO]: iter=451 {Training loss}=1.8029571771621704\n",
      "2021-01-24 23:21:14,143 [nnabla][INFO]: iter=453 {Training loss}=1.8649290800094604\n",
      "2021-01-24 23:21:32,828 [nnabla][INFO]: iter=455 {Training loss}=1.8541063070297241\n",
      "2021-01-24 23:21:51,393 [nnabla][INFO]: iter=457 {Training loss}=1.7200343608856201\n",
      "2021-01-24 23:22:09,871 [nnabla][INFO]: iter=459 {Training loss}=1.8125582933425903\n",
      "2021-01-24 23:22:28,648 [nnabla][INFO]: iter=461 {Training loss}=1.7332453727722168\n",
      "2021-01-24 23:22:47,213 [nnabla][INFO]: iter=463 {Training loss}=1.9242193698883057\n",
      "2021-01-24 23:23:06,067 [nnabla][INFO]: iter=465 {Training loss}=1.8351540565490723\n",
      "2021-01-24 23:23:24,714 [nnabla][INFO]: iter=467 {Training loss}=2.1096444129943848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:12 | Loss:[[ 0.7116388 ]\n",
      " [ 0.6850746 ]\n",
      " [ 0.71641034]\n",
      " [ 0.83063436]\n",
      " [ 0.73361135]\n",
      " [ 2.176107  ]\n",
      " [ 0.7869606 ]\n",
      " [ 0.6218951 ]\n",
      " [ 1.2514929 ]\n",
      " [ 0.7980007 ]\n",
      " [ 3.2280536 ]\n",
      " [ 2.0387475 ]\n",
      " [ 2.8412437 ]\n",
      " [ 7.837415  ]\n",
      " [ 7.837415  ]\n",
      " [ 1.6210132 ]\n",
      " [ 1.7566357 ]\n",
      " [ 1.6210132 ]\n",
      " [ 1.9052929 ]\n",
      " [ 1.7710059 ]\n",
      " [ 1.4467145 ]\n",
      " [ 1.543197  ]\n",
      " [ 0.8184636 ]\n",
      " [ 0.9711407 ]\n",
      " [ 0.8230087 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 0.22732477]\n",
      " [ 0.33312818]\n",
      " [ 1.796114  ]\n",
      " [ 3.475043  ]\n",
      " [ 0.23892602]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 1.1638472 ]\n",
      " [ 1.6119634 ]\n",
      " [ 1.483773  ]\n",
      " [ 3.5036998 ]\n",
      " [ 1.7806609 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 1.3711417 ]\n",
      " [ 1.1299379 ]\n",
      " [ 0.5643488 ]\n",
      " [ 2.8182707 ]\n",
      " [ 1.898674  ]\n",
      " [ 0.16642047]\n",
      " [ 0.16225214]\n",
      " [ 0.11944486]\n",
      " [ 1.196442  ]\n",
      " [11.148197  ]\n",
      " [ 8.510544  ]\n",
      " [ 3.2077982 ]\n",
      " [ 3.2077982 ]\n",
      " [ 3.2077982 ]\n",
      " [ 3.2077982 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 0.623537  ]\n",
      " [ 0.24960805]\n",
      " [ 0.37092006]\n",
      " [ 0.68779165]\n",
      " [ 0.78930366]\n",
      " [ 1.6748401 ]\n",
      " [ 0.495735  ]\n",
      " [ 1.5217309 ]\n",
      " [ 0.46674693]\n",
      " [ 1.1010845 ]\n",
      " [ 0.7148215 ]\n",
      " [ 0.76070005]\n",
      " [ 0.7700403 ]\n",
      " [ 0.9001393 ]\n",
      " [ 0.87639683]\n",
      " [ 1.0686166 ]\n",
      " [ 1.1137637 ]\n",
      " [ 2.4367096 ]\n",
      " [ 0.79439765]\n",
      " [ 1.1436884 ]\n",
      " [ 1.1945399 ]\n",
      " [ 0.7926864 ]\n",
      " [ 1.8304375 ]\n",
      " [ 0.80466205]\n",
      " [ 1.2505801 ]\n",
      " [ 0.17736563]\n",
      " [ 0.67721313]\n",
      " [ 1.4491061 ]\n",
      " [ 0.27185515]\n",
      " [ 0.6050595 ]\n",
      " [ 1.4121637 ]\n",
      " [ 2.207193  ]\n",
      " [ 0.7432534 ]\n",
      " [ 1.2616541 ]\n",
      " [ 0.6903021 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 0.472646  ]\n",
      " [ 0.85505486]\n",
      " [ 0.38927537]\n",
      " [ 0.22780918]\n",
      " [ 0.26201084]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 0.10877615]\n",
      " [ 1.61491   ]\n",
      " [ 1.8394368 ]\n",
      " [ 0.07825984]\n",
      " [ 0.1464585 ]\n",
      " [ 0.89757264]\n",
      " [ 0.29092732]\n",
      " [ 0.5553576 ]\n",
      " [ 0.2655831 ]\n",
      " [ 0.3396857 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.1034734 ]\n",
      " [ 0.7021928 ]\n",
      " [ 0.8953643 ]\n",
      " [ 2.395669  ]\n",
      " [ 0.65588063]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 0.45840976]\n",
      " [ 0.3926416 ]\n",
      " [ 1.1827651 ]\n",
      " [ 0.55962646]\n",
      " [ 0.55022895]\n",
      " [ 2.1206217 ]\n",
      " [ 0.83900386]\n",
      " [ 1.0637652 ]\n",
      " [ 4.765048  ]\n",
      " [ 1.135533  ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 2.7532513 ]\n",
      " [ 1.1832837 ]\n",
      " [ 1.9784367 ]\n",
      " [ 1.368098  ]\n",
      " [ 6.6250496 ]\n",
      " [ 6.6250496 ]\n",
      " [ 6.599942  ]\n",
      " [ 3.2128234 ]\n",
      " [ 3.2128234 ]\n",
      " [ 3.2128234 ]\n",
      " [ 3.2128234 ]\n",
      " [ 2.2222452 ]\n",
      " [ 2.873622  ]\n",
      " [ 2.8818398 ]\n",
      " [ 8.498146  ]\n",
      " [ 8.498146  ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-24 23:23:43,387 [nnabla][INFO]: iter=469 {Training loss}=1.69423246383667\n",
      "2021-01-24 23:24:02,046 [nnabla][INFO]: iter=471 {Training loss}=1.632779598236084\n",
      "2021-01-24 23:24:20,271 [nnabla][INFO]: iter=473 {Training loss}=1.7093065977096558\n",
      "2021-01-24 23:24:38,769 [nnabla][INFO]: iter=475 {Training loss}=1.5712509155273438\n",
      "2021-01-24 23:24:57,216 [nnabla][INFO]: iter=477 {Training loss}=1.7539420127868652\n",
      "2021-01-24 23:25:15,591 [nnabla][INFO]: iter=479 {Training loss}=1.5886414051055908\n",
      "2021-01-24 23:25:34,054 [nnabla][INFO]: iter=481 {Training loss}=1.6360946893692017\n",
      "2021-01-24 23:25:52,737 [nnabla][INFO]: iter=483 {Training loss}=1.7244871854782104\n",
      "2021-01-24 23:26:11,313 [nnabla][INFO]: iter=485 {Training loss}=1.4276334047317505\n",
      "2021-01-24 23:26:29,898 [nnabla][INFO]: iter=487 {Training loss}=1.5596362352371216\n",
      "2021-01-24 23:26:48,176 [nnabla][INFO]: iter=489 {Training loss}=1.6498373746871948\n",
      "2021-01-24 23:27:06,800 [nnabla][INFO]: iter=491 {Training loss}=1.5321471691131592\n",
      "2021-01-24 23:27:25,748 [nnabla][INFO]: iter=493 {Training loss}=1.4473823308944702\n",
      "2021-01-24 23:27:44,216 [nnabla][INFO]: iter=495 {Training loss}=1.5485448837280273\n",
      "2021-01-24 23:28:02,979 [nnabla][INFO]: iter=497 {Training loss}=1.594749093055725\n",
      "2021-01-24 23:28:21,582 [nnabla][INFO]: iter=499 {Training loss}=1.6447049379348755\n",
      "2021-01-24 23:28:39,783 [nnabla][INFO]: iter=501 {Training loss}=1.7239751815795898\n",
      "2021-01-24 23:28:58,360 [nnabla][INFO]: iter=503 {Training loss}=2.00333309173584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:13 | Loss:[[ 0.3053896 ]\n",
      " [ 0.2865725 ]\n",
      " [ 0.3352281 ]\n",
      " [ 0.6801262 ]\n",
      " [ 0.39217782]\n",
      " [ 2.2353296 ]\n",
      " [ 0.9444619 ]\n",
      " [ 0.68917483]\n",
      " [ 0.85367936]\n",
      " [ 0.8399429 ]\n",
      " [ 2.8281317 ]\n",
      " [ 2.222795  ]\n",
      " [ 2.6369863 ]\n",
      " [ 6.6993074 ]\n",
      " [ 6.6993074 ]\n",
      " [ 1.2403482 ]\n",
      " [ 1.1170275 ]\n",
      " [ 1.2403482 ]\n",
      " [ 1.5931108 ]\n",
      " [ 1.1148431 ]\n",
      " [ 1.0489922 ]\n",
      " [ 1.1567185 ]\n",
      " [ 0.8509086 ]\n",
      " [ 1.129875  ]\n",
      " [ 1.1058496 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 0.5054905 ]\n",
      " [ 0.38230458]\n",
      " [ 1.4645227 ]\n",
      " [ 2.2695434 ]\n",
      " [ 0.50324494]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 0.72924453]\n",
      " [ 1.6606367 ]\n",
      " [ 1.1764138 ]\n",
      " [ 1.3625606 ]\n",
      " [ 1.2766297 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 0.98366106]\n",
      " [ 1.117377  ]\n",
      " [ 0.84331316]\n",
      " [ 2.2309494 ]\n",
      " [ 2.3012295 ]\n",
      " [ 0.35343158]\n",
      " [ 0.73436123]\n",
      " [ 0.36140427]\n",
      " [ 0.24699351]\n",
      " [10.24211   ]\n",
      " [ 7.2838607 ]\n",
      " [ 3.1402252 ]\n",
      " [ 3.1402252 ]\n",
      " [ 3.1402252 ]\n",
      " [ 3.1402252 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 1.7751756 ]\n",
      " [ 0.24003124]\n",
      " [ 0.35152787]\n",
      " [ 1.1530919 ]\n",
      " [ 0.18911041]\n",
      " [ 1.840233  ]\n",
      " [ 0.99226195]\n",
      " [ 0.8847234 ]\n",
      " [ 0.7059224 ]\n",
      " [ 1.0563337 ]\n",
      " [ 0.2912577 ]\n",
      " [ 0.34666622]\n",
      " [ 0.2903805 ]\n",
      " [ 0.8009572 ]\n",
      " [ 1.6731484 ]\n",
      " [ 1.5858979 ]\n",
      " [ 1.1927221 ]\n",
      " [ 2.2417412 ]\n",
      " [ 1.669426  ]\n",
      " [ 1.009513  ]\n",
      " [ 0.9482281 ]\n",
      " [ 1.6562705 ]\n",
      " [ 1.546972  ]\n",
      " [ 0.6390244 ]\n",
      " [ 0.7031786 ]\n",
      " [ 0.52776235]\n",
      " [ 0.9202138 ]\n",
      " [ 0.39062655]\n",
      " [ 0.28354886]\n",
      " [ 1.0181632 ]\n",
      " [ 0.44077802]\n",
      " [ 0.31649548]\n",
      " [ 1.6659005 ]\n",
      " [ 0.34208626]\n",
      " [ 0.24266021]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 0.08971667]\n",
      " [ 0.75694513]\n",
      " [ 0.09678969]\n",
      " [ 0.05160528]\n",
      " [ 0.06984007]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 0.6138232 ]\n",
      " [ 0.34362257]\n",
      " [ 0.7912449 ]\n",
      " [ 0.3069083 ]\n",
      " [ 0.29272994]\n",
      " [ 0.27164504]\n",
      " [ 0.19472222]\n",
      " [ 0.26644668]\n",
      " [ 0.58232486]\n",
      " [ 0.14344399]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 0.78235674]\n",
      " [ 0.5731638 ]\n",
      " [ 1.588295  ]\n",
      " [ 0.93190986]\n",
      " [ 0.95718235]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 0.77990633]\n",
      " [ 0.55414975]\n",
      " [ 1.9656934 ]\n",
      " [ 0.6665597 ]\n",
      " [ 0.6819524 ]\n",
      " [ 3.031267  ]\n",
      " [ 2.187776  ]\n",
      " [ 1.4840814 ]\n",
      " [ 3.263198  ]\n",
      " [ 1.4320046 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 2.7609942 ]\n",
      " [ 1.5152864 ]\n",
      " [ 1.0765687 ]\n",
      " [ 1.3169286 ]\n",
      " [ 7.170102  ]\n",
      " [ 7.170102  ]\n",
      " [ 7.059765  ]\n",
      " [ 3.1577702 ]\n",
      " [ 3.1577702 ]\n",
      " [ 3.1577702 ]\n",
      " [ 3.1577702 ]\n",
      " [ 2.7083683 ]\n",
      " [ 2.2563438 ]\n",
      " [ 2.209375  ]\n",
      " [ 6.881482  ]\n",
      " [ 6.881482  ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-24 23:29:17,139 [nnabla][INFO]: iter=505 {Training loss}=1.5910378694534302\n",
      "2021-01-24 23:29:35,618 [nnabla][INFO]: iter=507 {Training loss}=1.5047122240066528\n",
      "2021-01-24 23:29:53,921 [nnabla][INFO]: iter=509 {Training loss}=1.5091701745986938\n",
      "2021-01-24 23:30:12,682 [nnabla][INFO]: iter=511 {Training loss}=1.371121883392334\n",
      "2021-01-24 23:30:31,419 [nnabla][INFO]: iter=513 {Training loss}=1.7639758586883545\n",
      "2021-01-24 23:30:50,316 [nnabla][INFO]: iter=515 {Training loss}=1.5457061529159546\n",
      "2021-01-24 23:31:08,450 [nnabla][INFO]: iter=517 {Training loss}=1.5162975788116455\n",
      "2021-01-24 23:31:26,896 [nnabla][INFO]: iter=519 {Training loss}=1.625199556350708\n",
      "2021-01-24 23:31:45,116 [nnabla][INFO]: iter=521 {Training loss}=1.424853801727295\n",
      "2021-01-24 23:32:03,448 [nnabla][INFO]: iter=523 {Training loss}=1.5448483228683472\n",
      "2021-01-24 23:32:22,202 [nnabla][INFO]: iter=525 {Training loss}=1.6317050457000732\n",
      "2021-01-24 23:32:40,714 [nnabla][INFO]: iter=527 {Training loss}=1.5575454235076904\n",
      "2021-01-24 23:32:59,242 [nnabla][INFO]: iter=529 {Training loss}=1.3954805135726929\n",
      "2021-01-24 23:33:17,839 [nnabla][INFO]: iter=531 {Training loss}=1.5009335279464722\n",
      "2021-01-24 23:33:36,478 [nnabla][INFO]: iter=533 {Training loss}=1.498183250427246\n",
      "2021-01-24 23:33:54,665 [nnabla][INFO]: iter=535 {Training loss}=1.517423391342163\n",
      "2021-01-24 23:34:13,009 [nnabla][INFO]: iter=537 {Training loss}=1.593572735786438\n",
      "2021-01-24 23:34:31,132 [nnabla][INFO]: iter=539 {Training loss}=1.9326226711273193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:14 | Loss:[[0.5170929 ]\n",
      " [0.50596356]\n",
      " [0.555598  ]\n",
      " [0.95067436]\n",
      " [0.604603  ]\n",
      " [2.1101642 ]\n",
      " [1.2945197 ]\n",
      " [1.2057016 ]\n",
      " [1.1543119 ]\n",
      " [2.060532  ]\n",
      " [2.8366601 ]\n",
      " [1.8213336 ]\n",
      " [1.7814302 ]\n",
      " [6.4951897 ]\n",
      " [6.4951897 ]\n",
      " [1.5829678 ]\n",
      " [1.4180293 ]\n",
      " [1.5829678 ]\n",
      " [1.7163961 ]\n",
      " [1.5448283 ]\n",
      " [1.05458   ]\n",
      " [1.2540704 ]\n",
      " [0.87264705]\n",
      " [1.1137701 ]\n",
      " [0.9430396 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [0.27886555]\n",
      " [0.23100016]\n",
      " [0.5787486 ]\n",
      " [1.7952925 ]\n",
      " [0.73052746]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [0.8705229 ]\n",
      " [1.3952467 ]\n",
      " [1.114162  ]\n",
      " [2.9941938 ]\n",
      " [1.1525978 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [1.3653902 ]\n",
      " [1.9242564 ]\n",
      " [0.6001792 ]\n",
      " [2.2889712 ]\n",
      " [2.1261046 ]\n",
      " [0.08699316]\n",
      " [1.0400984 ]\n",
      " [0.14370887]\n",
      " [0.18981615]\n",
      " [9.794252  ]\n",
      " [6.93735   ]\n",
      " [3.1451118 ]\n",
      " [3.1451118 ]\n",
      " [3.1451118 ]\n",
      " [3.1451118 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [0.10264893]\n",
      " [0.15829512]\n",
      " [0.1117992 ]\n",
      " [0.78384477]\n",
      " [1.9800507 ]\n",
      " [1.4725634 ]\n",
      " [0.5808544 ]\n",
      " [1.1203716 ]\n",
      " [0.47435543]\n",
      " [3.4135368 ]\n",
      " [0.49370572]\n",
      " [0.49418613]\n",
      " [0.61322814]\n",
      " [1.1042908 ]\n",
      " [1.0375215 ]\n",
      " [0.55029184]\n",
      " [0.94401383]\n",
      " [1.5288055 ]\n",
      " [1.7736802 ]\n",
      " [1.3596575 ]\n",
      " [0.99174976]\n",
      " [0.7443013 ]\n",
      " [1.3208833 ]\n",
      " [0.6672584 ]\n",
      " [0.67734534]\n",
      " [1.0994525 ]\n",
      " [0.11210541]\n",
      " [0.13315043]\n",
      " [0.2440195 ]\n",
      " [0.57039154]\n",
      " [0.39334884]\n",
      " [0.55998206]\n",
      " [0.9643497 ]\n",
      " [0.7951398 ]\n",
      " [0.33424342]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [0.14036131]\n",
      " [0.3198196 ]\n",
      " [0.10653146]\n",
      " [0.09257549]\n",
      " [0.14198689]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [0.05341404]\n",
      " [0.05558631]\n",
      " [0.35273856]\n",
      " [0.07324756]\n",
      " [0.10127161]\n",
      " [0.21161726]\n",
      " [0.10330345]\n",
      " [0.20817865]\n",
      " [0.08522601]\n",
      " [0.20636569]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [0.97684085]\n",
      " [1.054808  ]\n",
      " [0.65631765]\n",
      " [0.584373  ]\n",
      " [1.0139996 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [0.571569  ]\n",
      " [0.49842316]\n",
      " [1.4572594 ]\n",
      " [0.85933363]\n",
      " [0.58447444]\n",
      " [0.5128903 ]\n",
      " [0.7555121 ]\n",
      " [0.6172403 ]\n",
      " [3.5596273 ]\n",
      " [1.2676826 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [2.7656524 ]\n",
      " [0.7008936 ]\n",
      " [2.3591728 ]\n",
      " [0.67751384]\n",
      " [7.225131  ]\n",
      " [7.225131  ]\n",
      " [4.528116  ]\n",
      " [3.1506813 ]\n",
      " [3.1506813 ]\n",
      " [3.1506813 ]\n",
      " [3.1506813 ]\n",
      " [1.4176614 ]\n",
      " [1.6921675 ]\n",
      " [1.6688533 ]\n",
      " [6.1062584 ]\n",
      " [6.1062584 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-24 23:34:49,699 [nnabla][INFO]: iter=541 {Training loss}=1.4901132583618164\n",
      "2021-01-24 23:35:08,042 [nnabla][INFO]: iter=543 {Training loss}=1.4873501062393188\n",
      "2021-01-24 23:35:26,338 [nnabla][INFO]: iter=545 {Training loss}=1.4416736364364624\n",
      "2021-01-24 23:35:44,443 [nnabla][INFO]: iter=547 {Training loss}=1.394999384880066\n",
      "2021-01-24 23:36:02,587 [nnabla][INFO]: iter=549 {Training loss}=1.5797622203826904\n",
      "2021-01-24 23:36:20,374 [nnabla][INFO]: iter=551 {Training loss}=1.4178881645202637\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2cf35b0fbd89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0menc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-6b2534a09398>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[0mxi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m                 \u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclear_buffer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "enc = Encoder(dataset)\n",
    "enc.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers_per_batch, utterances_per_speaker = embeds.shape[:2]\n",
    "centroids_incl = F.mean(embeds, axis=1, keepdims=True)\n",
    "centroids_incl = centroids_incl / F.norm(centroids_incl, axis=2, keepdims=True)\n",
    "centroids_excl = F.sum(embeds, axis=1, keepdims=True) - embeds\n",
    "centroids_excl /= (utterances_per_speaker - 1)\n",
    "centroids_excl = centroids_excl / F.norm(centroids_excl, axis=2, keepdims=True)\n",
    "sim_matrix = nn.Variable.from_numpy_array(np.zeros((speakers_per_batch, utterances_per_speaker, speakers_per_batch),dtype=np.int))\n",
    "mask_matrix = 1 - np.eye(speakers_per_batch, dtype=np.int)\n",
    "for j in range(speakers_per_batch):\n",
    "    mask = np.where(mask_matrix[j])[0]\n",
    "    sim_matrix[mask, :, j] = F.sum(embeds[mask] * centroids_incl[j],axis=2)\n",
    "    sim_matrix[j, :, j] = F.sum(embeds[j] * centroids_excl[j],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center = F.mean(embedded_split, axis=1)             # [N,P] normalized center vectors eq.(1)\n",
    "center_norm = F.norm(center,2,keepdims=True)\n",
    "center = center/center_norm\n",
    "\n",
    "center_except = F.reshape(F.sum(embedded_split, axis=1, keepdims=True) - embedded_split, (N*M,P))  # [NM,P] center vectors eq.(8)\n",
    "# make similarity matrix eq.(9)\n",
    "S = tf.concat(\n",
    "    [tf.concat([tf.reduce_sum(center_except[i*M:(i+1)*M,:]*embedded_split[j,:,:], axis=1, keep_dims=True) if i==j\n",
    "                else tf.reduce_sum(center[i:(i+1),:]*embedded_split[j,:,:], axis=1, keep_dims=True) for i in range(N)],\n",
    "               axis=1) for j in range(N)], axis=0)\n",
    "\n",
    "nnabla.functions.concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = {\"s1\":np.array([np.random.randint(1,10,(2,4)),np.random.randint(1,10,(2,4)),\n",
    "                     np.random.randint(1,10,(2,4)),np.random.randint(1,10,(2,4))]),\n",
    "      \"s2\":np.array([np.random.randint(1,10,(2,4)),np.random.randint(1,10,(2,4)),\n",
    "                     np.random.randint(1,10,(2,4)),np.random.randint(1,10,(2,4))])    \n",
    "     \n",
    "     }\n",
    "zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = zz.copy()\n",
    "curr= xx[\"s1\"][0:2]\n",
    "print(xx[\"s1\"].shape)\n",
    "xx[\"s1\"]=np.delete(xx[\"s1\"], [0,1], axis=0)\n",
    "xx[\"s1\"]=np.append(xx[\"s1\"], curr, axis=0)\n",
    "print(xx[\"s1\"].shape)\n",
    "print(curr)\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.set_auto_forward(True)\n",
    "\n",
    "def compute_similarity(emb, n_speakers, n_utterances):\n",
    "    # embedding reshape\n",
    "    emb_re = emb.reshape((n_speakers, n_utterances, -1))\n",
    "    \n",
    "    # compute the inclusion centroids\n",
    "    cen = F.mean(emb_re, axis=1) \n",
    "    cen = cen / F.norm(cen, axis=1, keepdims=True)\n",
    "    cen = F.reshape(cen, (-1, emb.shape[-1]))\n",
    "    \n",
    "    # compute the exclusion centroids\n",
    "    exc = F.sum(emb_re, axis=1, keepdims=True) - emb_re\n",
    "    exc = exc / (n_utterances - 1)\n",
    "    exc = exc / F.norm(exc, axis=2, keepdims=True)\n",
    "    exc = F.reshape(exc, emb.shape)\n",
    "\n",
    "    diag = F.sum(exc * emb, axis=1, keepdims=True) # 20 x 1\n",
    "    sim = F.affine(emb,  F.transpose(cen, (1, 0))) \n",
    "\n",
    "    mask = np.concatenate([np.tile(w, (n_utterances, 1)) for w in np.eye(n_speakers)])\n",
    "    mask = nn.Variable.from_numpy_array(mask)\n",
    "    ret = (1 - mask) * sim + F.tile(diag, n_speakers) * mask\n",
    "    return  ret\n",
    "\n",
    "\n",
    "\n",
    "bz = 5\n",
    "dim = 6\n",
    "n_speakers = 5\n",
    "n_utterances = 4\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "data = np.arange(bz * n_utterances * dim).reshape((bz * n_utterances, dim))\n",
    "\n",
    "rng.randn(bz, dim)\n",
    "\n",
    "emb = nn.Variable.from_numpy_array(data)\n",
    "emb = emb / F.norm(emb, axis=1, keepdims=True)\n",
    "\n",
    "ret =  compute_similarity(emb, n_speakers, n_utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8931736 , 0.85282373, 0.8417678 , 0.8371577 , 0.8346132 ],\n",
       "       [0.99777466, 0.9890098 , 0.98571754, 0.98425335, 0.98342335],\n",
       "       [0.9846763 , 0.99773246, 0.99611515, 0.99533224, 0.99487424],\n",
       "       [0.975716  , 0.99944794, 0.9985397 , 0.99804544, 0.997745  ],\n",
       "       [0.9831892 , 0.9998139 , 0.99937737, 0.9990421 , 0.99882853],\n",
       "       [0.98095524, 0.9999939 , 0.99972546, 0.9994907 , 0.99933195],\n",
       "       [0.9793355 , 0.99997264, 0.99988353, 0.99971807, 0.99959725],\n",
       "       [0.9781107 , 0.9998819 , 0.99995685, 0.999842  , 0.99974895],\n",
       "       [0.9771528 , 0.99987066, 0.99998003, 0.9999125 , 0.99984056],\n",
       "       [0.97638404, 0.9998066 , 0.9999988 , 0.99995357, 0.9998982 ],\n",
       "       [0.97575355, 0.99974537, 0.99999726, 0.99997735, 0.9999353 ],\n",
       "       [0.9752273 , 0.99968845, 0.99998474, 0.9999905 , 0.99995965],\n",
       "       [0.97478133, 0.9996364 , 0.9999811 , 0.9999952 , 0.9999757 ],\n",
       "       [0.9743989 , 0.99958897, 0.9999692 , 0.99999964, 0.99998623],\n",
       "       [0.9740671 , 0.99954563, 0.9999565 , 0.9999994 , 0.99999285],\n",
       "       [0.97377664, 0.9995062 , 0.9999438 , 0.999996  , 0.9999969 ],\n",
       "       [0.9735202 , 0.99947023, 0.9999312 , 0.99999464, 0.9999982 ],\n",
       "       [0.97329223, 0.9994374 , 0.999919  , 0.999991  , 0.9999998 ],\n",
       "       [0.97308826, 0.9994073 , 0.9999074 , 0.9999869 , 0.9999999 ],\n",
       "       [0.9729045 , 0.9993796 , 0.9998963 , 0.9999825 , 0.9999985 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.concatenate([np.tile(w, (4, 1)) for w in np.eye(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1.],\n",
       "       [0., 1., 1., 1., 1.],\n",
       "       [0., 1., 1., 1., 1.],\n",
       "       [0., 1., 1., 1., 1.],\n",
       "       [1., 0., 1., 1., 1.],\n",
       "       [1., 0., 1., 1., 1.],\n",
       "       [1., 0., 1., 1., 1.],\n",
       "       [1., 0., 1., 1., 1.],\n",
       "       [1., 1., 0., 1., 1.],\n",
       "       [1., 1., 0., 1., 1.],\n",
       "       [1., 1., 0., 1., 1.],\n",
       "       [1., 1., 0., 1., 1.],\n",
       "       [1., 1., 1., 0., 1.],\n",
       "       [1., 1., 1., 0., 1.],\n",
       "       [1., 1., 1., 0., 1.],\n",
       "       [1., 1., 1., 0., 1.],\n",
       "       [1., 1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
